[{"authors":["admin"],"categories":null,"content":"I am David Zimmermann, a data scientist from Cologne, Germany interested in solving real-world problems using machine learning, data science, and other appropriate measures. In my past, I did a PhD in computational finance, where I simulated financial markets using agent-based models to find the effects of high-frequency trading.\nThis blog was built with the intention to share different applications, tricks, and also some mild shenanigans with data, R, C++, Python, or other areas. If you have any comments, ideas, issues, or just want to give me a thumbs up, leave a note or write an email to david_j_zimmermann {at] hotmail [dot} com.\nThe source of this blog can be found on my .\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://davzim.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am David Zimmermann, a data scientist from Cologne, Germany interested in solving real-world problems using machine learning, data science, and other appropriate measures. In my past, I did a PhD in computational finance, where I simulated financial markets using agent-based models to find the effects of high-frequency trading.","tags":null,"title":"David Zimmermann, PhD","type":"authors"},{"authors":[],"categories":null,"content":"","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://davzim.github.io/talk/example/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"Work in Progress, will be updated shortly","tags":[],"title":"WIP","type":"talk"},{"authors":null,"categories":["R"],"content":"\rIn every data project, there should be a check that the data actually looks like what you expect it to look like.\rThis can be as simple as stopifnot(all(data$values \u0026gt; 0)), but as with everything “simple”, you typically want to have some additional features, such as cleaner error messages, rules separated from your R script (eg in a yaml file), result visualization, and last but least, a library that does this as fast as possible.\rThe last bit is especially important when you have the data in a database or as an arrow .parquet file, due to its size or complexity.\nThe newly released dataverifyr package (on CRAN) allows you to do exactly that: write rules in a yaml file, load the rules, check if the rules where matched and filter for data points that do not conform to the rules; all while respecting your data framework: data.table, dplyr, arrow, duckdb, or other DBI-compliant databases.\nThe following is an excerpt of the Readme of ´dataverifyr` that highlights how to use the package.\nLarger Example using the arrow backend\rFor a more involved example, using a different backend, let’s say we\rhave a larger dataset of taxi trips from NY (see also the official\rsource of the\rdata),\rthat we have saved as a local arrow dataset (using parquet as a data\rformat), where we want to make sure that some variables are in-line with\rour expectations/rules.\n1 Download and Prepare Data\rFirst we prepare the data by downloading it and writing the dataset to\r.parquet files. This needs to be done only once and is shown for\rreproducibility reasons only, the actual dataverifyr code is shown\rbelow the next block\nlibrary(arrow)\rurl \u0026lt;- \u0026quot;https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-01.parquet\u0026quot;\rfile \u0026lt;- \u0026quot;yellow_tripdata_2018-01.parquet\u0026quot;\rif (!file.exists(file)) download.file(url, file, method = \u0026quot;curl\u0026quot;)\rfile.size(file) / 1e6 # in MB\r#\u0026gt; [1] 123.6685\r# quick check of the filesize\rd \u0026lt;- read_parquet(file)\rdim(d)\r#\u0026gt; [1] 8760687 19\rnames(d)\r#\u0026gt; [1] \u0026quot;VendorID\u0026quot; \u0026quot;tpep_pickup_datetime\u0026quot; \u0026quot;tpep_dropoff_datetime\u0026quot; \u0026quot;passenger_count\u0026quot; #\u0026gt; [5] \u0026quot;trip_distance\u0026quot; \u0026quot;RatecodeID\u0026quot; \u0026quot;store_and_fwd_flag\u0026quot; \u0026quot;PULocationID\u0026quot; #\u0026gt; [9] \u0026quot;DOLocationID\u0026quot; \u0026quot;payment_type\u0026quot; \u0026quot;fare_amount\u0026quot; \u0026quot;extra\u0026quot; #\u0026gt; [13] \u0026quot;mta_tax\u0026quot; \u0026quot;tip_amount\u0026quot; \u0026quot;tolls_amount\u0026quot; \u0026quot;improvement_surcharge\u0026quot;\r#\u0026gt; [17] \u0026quot;total_amount\u0026quot; \u0026quot;congestion_surcharge\u0026quot; \u0026quot;airport_fee\u0026quot;\r# write the dataset to disk\rwrite_dataset(d, \u0026quot;nyc-taxi-data\u0026quot;)\r2 Create Rules in yaml\rNext, we can create some rules that we will use to check our data. As we\rsaw earlier, we can create the rules in R using the rule() and\rruleset() functions, there is however, the (in my opinion) preferred\roption to separate the code from the rules by writing the rules in a\rseparate yaml file and reading them into R.\nFirst we display the hand-written contents of the nyc_data_rules.yaml\rfile.\n- name: \u0026#39;Rule for: passenger_count\u0026#39;\rexpr: passenger_count \u0026gt;= 0 \u0026amp; passenger_count \u0026lt;= 10\rallow_na: no\rnegate: no\rindex: 1\r- name: \u0026#39;Rule for: trip_distance\u0026#39;\rexpr: trip_distance \u0026gt;= 0 \u0026amp; trip_distance \u0026lt;= 1000\rallow_na: no\rnegate: no\rindex: 2\r- name: \u0026#39;Rule for: payment_type\u0026#39;\rexpr: payment_type %in% c(0, 1, 2, 3, 4)\rallow_na: no\rnegate: no\rindex: 3\rThen, we can load, display, and finally check the rules against the data\nrules \u0026lt;- read_rules(\u0026quot;nyc_data_rules.yaml\u0026quot;)\rrules\r#\u0026gt; \u0026lt;Verification Ruleset with 3 elements\u0026gt;\r#\u0026gt; [1] \u0026#39;Rule for: passenger_count\u0026#39; matching `passenger_count \u0026gt;= 0 \u0026amp; passenger_count \u0026lt;= 10` (allow_na: FALSE)\r#\u0026gt; [2] \u0026#39;Rule for: trip_distance\u0026#39; matching `trip_distance \u0026gt;= 0 \u0026amp; trip_distance \u0026lt;= 1000` (allow_na: FALSE)\r#\u0026gt; [3] \u0026#39;Rule for: payment_type\u0026#39; matching `payment_type %in% c(0, 1, 2, 3, 4)` (allow_na: FALSE)\r3 Verify that the Data matches the given Rules\rNow we can check if the data follows our rules or if we have unexpected\rdata points:\n# open the dataset ds \u0026lt;- open_dataset(\u0026quot;nyc-taxi-data/\u0026quot;)\r# perform the data validation check\rres \u0026lt;- check_data(ds, rules)\rres\r#\u0026gt; # A tibble: 3 × 10\r#\u0026gt; name expr allow…¹ negate tests pass fail warn error time #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;drt\u0026gt;\r#\u0026gt; 1 Rule for: passenger_count passenger_count … FALSE FALSE 8760687 8760687 0 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 0.56…\r#\u0026gt; 2 Rule for: trip_distance trip_distance \u0026gt;=… FALSE FALSE 8760687 8760686 1 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 0.43…\r#\u0026gt; 3 Rule for: payment_type payment_type %in… FALSE FALSE 8760687 8760687 0 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 0.42…\r#\u0026gt; # … with abbreviated variable name ¹​allow_na\rplot_res(res)\rUsing the power of arrow, we were able to scan 8+mln observations for\rthree rules in about 1.5 seconds (YMMV). As we can see from the results,\rthere is one unexpected value, lets quickly investigate using the\rfilter_fails() function, which filters a dataset for the failed rule\rmatches\nres |\u0026gt;\rfilter_fails(ds) |\u0026gt; # only select a couple of variables for brevity\rdplyr::select(tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance)\r#\u0026gt; # A tibble: 1 × 3\r#\u0026gt; tpep_pickup_datetime tpep_dropoff_datetime trip_distance\r#\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 2018-01-30 12:41:02 2018-01-30 12:42:09 189484.\rAs we can see, this is probably a data error (a trip distance of 190k\rmiles in 1 minute seems - ehm stellar…).\nUsing a DBI Backend\rIf you have a SQLite or duckdb database, you can use the package\rlike this\nlibrary(DBI)\rlibrary(dplyr)\r# connect to a duckdb database\rcon \u0026lt;- dbConnect(duckdb::duckdb(\u0026quot;duckdb-database.duckdb\u0026quot;))\r# for demo purposes write the data once\rdbWriteTable(con, \u0026quot;mtcars\u0026quot;, mtcars)\r# create a tbl connection, which can be used in the checks\rtbl \u0026lt;- tbl(con, \u0026quot;mtcars\u0026quot;)\r# create rules\rrules \u0026lt;- ruleset(\rrule(mpg \u0026gt; 10 \u0026amp; mpg \u0026lt; 30),\rrule(cyl %in% c(4, 8)),\rrule(vs %in% c(0, 1), allow_na = TRUE)\r)\r# check rules\rres \u0026lt;- check_data(tbl, rules)\rres\r#\u0026gt; # A tibble: 3 × 10\r#\u0026gt; name expr allow_na negate tests pass fail warn error time #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;drtn\u0026gt; #\u0026gt; 1 Rule for: mpg mpg \u0026gt; 10 \u0026amp; mpg \u0026lt; 30 FALSE FALSE 32 28 4 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 3.5227668 secs\r#\u0026gt; 2 Rule for: cyl cyl %in% c(4, 8) FALSE FALSE 32 25 7 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 0.2015200 secs\r#\u0026gt; 3 Rule for: vs vs %in% c(0, 1) TRUE FALSE 32 32 0 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 0.1898661 secs\rfilter_fails(res, tbl, per_rule = TRUE)\r#\u0026gt; $`mpg \u0026gt; 10 \u0026amp; mpg \u0026lt; 30`\r#\u0026gt; # A tibble: 4 × 11\r#\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 32.4 4 78.7 66 4.08 2.2 19.47 1 1 4 1\r#\u0026gt; 2 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2\r#\u0026gt; 3 33.9 4 71.1 65 4.22 1.835 19.9 1 1 4 1\r#\u0026gt; 4 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2\r#\u0026gt; #\u0026gt; $`cyl %in% c(4, 8)`\r#\u0026gt; # A tibble: 7 × 11\r#\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 21 6 160 110 3.9 2.62 16.46 0 1 4 4\r#\u0026gt; 2 21 6 160 110 3.9 2.875 17.02 0 1 4 4\r#\u0026gt; 3 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1\r#\u0026gt; 4 18.1 6 225 105 2.76 3.46 20.22 1 0 3 1\r#\u0026gt; 5 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4\r#\u0026gt; 6 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4\r#\u0026gt; 7 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6\r# lastly disconnect from the database again\rdbDisconnect(con, shutdown = TRUE)\r","date":1689206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689206400,"objectID":"f8fd5b64d6f21813366f269918de5430","permalink":"https://davzim.github.io/blog/2023-07-13-dataverifyr/","publishdate":"2023-07-13T00:00:00Z","relpermalink":"/blog/2023-07-13-dataverifyr/","section":"blog","summary":"In every data project, there should be a check that the data actually looks like what you expect it to look like.\rThis can be as simple as stopifnot(all(data$values \u0026gt; 0)), but as with everything “simple”, you typically want to have some additional features, such as cleaner error messages, rules separated from your R script (eg in a yaml file), result visualization, and last but least, a library that does this as fast as possible.","tags":["R","package","verification","data"],"title":"Introducing dataverifyr: A Lightweight, Flexible, and Fast Data Validation Package that Can Handle All Sizes of Data ","type":"blog"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"787d95ef017995442819ac5b7f0b79d8","permalink":"https://davzim.github.io/project/ritch/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/ritch/","section":"project","summary":"An R package/interface to the ITCH Protocol for financial message data","tags":["Finance","Package"],"title":"RITCH","type":"project"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ce5f8266b6ce7aef281912988de8ae2d","permalink":"https://davzim.github.io/project/schellingr/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/schellingr/","section":"project","summary":"An R package to efficiently simulate Schellings Urban Migration Model","tags":["ABM","Package"],"title":"SchellingR","type":"project"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a81950393aa37e433afa0fc8e1e1a51e","permalink":"https://davzim.github.io/project/varsextra/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/varsextra/","section":"project","summary":"An R package to make handling VAR results easier","tags":["Package"],"title":"varsExtra","type":"project"},{"authors":null,"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"fbb7cdbb8bfe6e52d8d1830bb6f50e31","permalink":"https://davzim.github.io/project/colortable/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/colortable/","section":"project","summary":"A Package that makes it easy to color tables","tags":["Package"],"title":"colorTable","type":"project"},{"authors":null,"categories":["R"],"content":"\rRecently I was faced with a file compressed in NASDAQ’s ITCH-protocol, as I wasn’t able to find an R-package that parses and loads the file to R for me, I spent (probably) way to much time to write one, so here it is.\rBut you might wonder, what exactly is ITCH and why should I care? Well, ITCH is the outbound protocol NASDAQ uses to communicate market data to its clients, that is, all information including market status, orders, trades, circuit breakers, etc.\rwith nanosecond timestamps for each day and each exchange.\rKinda a must-have if you are looking into market microstructure, a good-to-have-looked-into-it if you are interested in general finance and/or if you are interested in data analytics and large structured datasets.\rIf you are wondering where you might get some of these fancy datasets in the first place, I have good news for you.\rNASDAQ provides some sample datasets (6 days for 3 exchanges (NASDAQ, PSX, and BX), together about 25GBs gzipped) on its FTP server: ftp://emi.nasdaq.com/ITCH/\nThe RITCH Package\rNow that I (hopefully) have your attention, let me represent to you RITCH an R package that parses ITCH-files (version 5.0).\rCurrently the package only lives on GitHub (https://github.com/DavZim/RITCH), but it should find its way into CRAN eventually.\rUntil then, you have to use devtools or remotes to install it\n# install.packages(\u0026quot;remotes\u0026quot;)\rremotes::install_github(\u0026quot;DavZim/RITCH\u0026quot;)\rAnd you should be good to go (if not, please let me know!).\rRITCH so far has a very limited scope: extracting the messages from the ITCH-file plus some functions to count messages.\rThe package leverages C++ and the excellent Rcpp library to optimise parsing.\rRITCH itself does not contain any data as the datasets are too large for any repos and I have no copyright on the datasets in any way.\rFor the following code I will use the 20170130.BX_ITCH_50-file from NASDAQ’s FTP-server, as its not too large at 714MB gzipped (1.6GB gunzipped), but still has almost 55 million messages.\nAll functions can take the gzipped or unzipped files, but if you use the file more than once and hard-drive space is not of utmost concern, I suggest you gunzip the file by hand (i.e., use R.utils::gunzip(file, new_file, remove = FALSE) in R or gunzip -k YYYYMMDD.XXX_ITCH_50.gz in the terminal) and call the functions on the “plain”-file.\rI will address some concerns to size and speed later on.\nTo download and prepare the data in R, we can use the following code\nfile \u0026lt;- \u0026quot;20170130.BX_ITCH_50.gz\u0026quot;\r# might take some time as it downloads 714MB\rif (!file.exists(file)) download.file(\u0026quot;ftp://emi.nasdaq.com/ITCH/20170130.BX_ITCH_50.gz\u0026quot;, file, mode = \u0026quot;wb\u0026quot;)\r# gunzip the file, but keep the original file\rR.utils::gunzip(\u0026quot;20170130.BX_ITCH_50.gz\u0026quot;, \u0026quot;20170130.BX_ITCH_50\u0026quot;, remove = FALSE)\rFirst, we want to get a general overview of the file, which we can do with count_messages()\nlibrary(RITCH)\rfile \u0026lt;- \u0026quot;20170130.BX_ITCH_50\u0026quot;\rmsg_count \u0026lt;- count_messages(file, add_meta_data = TRUE) #\u0026gt; [Counting] 54473386 messages found\r#\u0026gt; [Converting] to data.table\rmsg_count\r#\u0026gt; msg_type count msg_name msg_group doc_nr\r#\u0026gt; 1: S 6 System Event Message System Event Message 4.1\r#\u0026gt; 2: R 8371 Stock Directory Stock Related Messages 4.2.1\r#\u0026gt; 3: H 8401 Stock Trading Action Stock Related Messages 4.2.2\r#\u0026gt; 4: Y 8502 Reg SHO Restriction Stock Related Messages 4.2.3\r#\u0026gt; 5: L 6011 Market Participant Position Stock Related Messages 4.2.4\r#\u0026gt; 6: V 2 MWCB Decline Level Message Stock Related Messages 4.2.5.1\r#\u0026gt; 7: W 0 MWCB Status Message Stock Related Messages 4.2.5.2\r#\u0026gt; 8: K 0 IPO Quoting Period Update Stock Related Messages 4.2.6\r#\u0026gt; 9: J 0 LULD Auction Collar Stock Related Messages 4.2.7\r#\u0026gt; 10: A 21142017 Add Order Message Add Order Message 4.3.1\r#\u0026gt; 11: F 20648 Add Order - MPID Attribution Message Add Order Message 4.3.2\r#\u0026gt; 12: E 1203625 Order Executed Message Modify Order Messages 4.4.1\r#\u0026gt; 13: C 8467 Order Executed Message With Price Message Modify Order Messages 4.4.2\r#\u0026gt; 14: X 1498904 Order Cancel Message Modify Order Messages 4.4.3\r#\u0026gt; 15: D 20282644 Order Delete Message Modify Order Messages 4.4.4\r#\u0026gt; 16: U 3020278 Order Replace Message Modify Order Messages 4.4.5\r#\u0026gt; 17: P 330023 Trade Message (Non-Cross) Trade Messages 4.5.1\r#\u0026gt; 18: Q 0 Cross Trade Message Trade Messages 4.5.2\r#\u0026gt; 19: B 0 Broken Trade Message Trade Messages 4.5.3\r#\u0026gt; 20: I 0 NOII Message Net Order Imbalance Indicator (NOII) Message 4.6\r#\u0026gt; 21: N 6935487 Retail Interest Message Retail Price Improvement Indicator (RPII) 4.7\r#\u0026gt; msg_type count msg_name msg_group doc_nr\rAs you can see, there are a lot of different message types.\nCurrently this package parses only messages from the group “Add Order Messages” (type ‘A’ and ‘F’), “Modify Order Messages” (type ‘E’, ‘C’, ‘X’, ‘D’, and ‘U’), and “Trade Messages” (type ‘P’, ‘Q’, and ‘B’).\rYou can extract the different message-types by using the functions get_orders, get_modifications, and get_trades, respectively.\rThe doc-number refers to the section in the official documentation (which also contains more detailed description what each type contains).\rIf you are annoyed by the feedback the function gives you ([Counting] ... [Converting]...), you can always turn the feedback off with the quiet = TRUE option (this applies to all functions).\rLets try to parse the first 10 orders\norders \u0026lt;- get_orders(file, 1, 10) #\u0026gt; 10 messages found\r#\u0026gt; [Loading] .\r#\u0026gt; [Converting] to data.table\r#\u0026gt; [Formatting]\rorders\r#\u0026gt; msg_type locate_code tracking_number timestamp order_ref buy shares stock price mpid date datetime\r#\u0026gt; 1: A 7584 0 2.520001e+13 36132 TRUE 500000 UAMY 0.0001 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 2: A 3223 0 2.520001e+13 36133 TRUE 500000 GLOW 0.0001 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 3: A 2937 0 2.520001e+13 36136 FALSE 200 FRP 18.6500 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 4: A 5907 0 2.520001e+13 36137 TRUE 1500 PIP 3.1500 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 5: A 5907 0 2.520001e+13 36138 FALSE 2000 PIP 3.2500 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 6: A 5907 0 2.520001e+13 36139 TRUE 3000 PIP 3.1000 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 7: A 5398 0 2.520001e+13 36140 TRUE 200 NSR 33.0000 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 8: A 5907 0 2.520001e+13 36141 FALSE 500 PIP 3.2500 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 9: A 2061 0 2.520001e+13 36142 FALSE 1300 DSCI 7.0000 NA 2017-01-30 2017-01-30 07:00:00\r#\u0026gt; 10: A 1582 0 2.520001e+13 36143 TRUE 500 CPPL 17.1500 NA 2017-01-30 2017-01-30 07:00:00\rThe same works for trades using the get_trades() function and for order modifications using the get_modifications() function.\rTo speed up the get_* functions, we can use the message-count information from earlier.\rFor example the following code yields the same results as above, but saves time.\norders \u0026lt;- get_orders(file, 1, count_orders(msg_count)) trades \u0026lt;- get_trades(file, 1, count_trades(msg_count)) If you want to get more information about each field, you can have a look at the official ITCH-protocol specification manual or you can get a small data.table about each message type by calling get_meta_data().\nHaving a Look at some the most traded ETFs\rTo have at least one piece of eye-candy in this post, lets have a quick go at the orders and trades of SPY (an S\u0026amp;P 500 ETF and one of the most traded assets, in case you didn’t know), IWO (Russel 2000 Growth ETF), IWM (Russel 2000 Index ETF), and VXX (S\u0026amp;P 500 VIX ETF) on the BX-exchange.\rIn case you are wondering, I got these four tickers with\nlibrary(magrittr)\rget_orders(file, 1, count_orders(msg_count), quiet = T) %\u0026gt;% .$stock %\u0026gt;% table %\u0026gt;% sort(decreasing = T) %\u0026gt;% head(4)\r#\u0026gt; .\r#\u0026gt; SPY IWO IWM VXX #\u0026gt; 135119 135016 118123 117395\rFirst we load the data (orders and trades) from the file, then we do some data munging, and finally plot the data using ggplot2.\nlibrary(ggplot2)\r# 0. load the data\rorders \u0026lt;- get_orders(file, 1, count_orders(msg_count))\r#\u0026gt; 21162665 messages found\r#\u0026gt; [Loading] ................\r#\u0026gt; [Converting] to data.table\r#\u0026gt; [Formatting]\rtrades \u0026lt;- get_trades(file, 1, count_trades(msg_count))\r#\u0026gt; 330023 messages found\r#\u0026gt; [Loading] ................\r#\u0026gt; [Converting] to data.table\r#\u0026gt; [Formatting]\r# 1. data munging\rtickers \u0026lt;- c(\u0026quot;SPY\u0026quot;, \u0026quot;IWO\u0026quot;, \u0026quot;IWM\u0026quot;, \u0026quot;VXX\u0026quot;)\rdt_orders \u0026lt;- orders[stock %in% tickers]\rdt_trades \u0026lt;- trades[stock %in% tickers]\r# for each ticker, use only orders that are within 1% of the range of traded prices\rranges \u0026lt;- dt_trades[, .(min_price = min(price), max_price = max(price)), by = stock]\r# filter the orders\rdt_orders \u0026lt;- dt_orders[ranges, on = \u0026quot;stock\u0026quot;][price \u0026gt;= 0.99 * min_price \u0026amp; price \u0026lt;= 1.01 * max_price]\r# replace the buy-factor with something more useful\rdt_orders[, buy := ifelse(buy, \u0026quot;Bid\u0026quot;, \u0026quot;Ask\u0026quot;)]\rdt_orders[, stock := factor(stock, levels = tickers)]\r# 2. data visualization\rggplot() +\r# add the orders to the plot\rgeom_point(data = dt_orders,\raes(x = datetime, y = price, color = buy), size = 0.5) +\r# add the trades as a black line to the plot\rgeom_step(data = dt_trades,\raes(x = datetime, y = price)) +\r# add a facet for each ETF\rfacet_wrap(~stock, scales = \u0026quot;free_y\u0026quot;) +\r# some Aesthetics\rtheme_light() +\rlabs(title = \u0026quot;Orders and Trades of the largest ETFs\u0026quot;,\rsubtitle = \u0026quot;Date: 2017-01-30 | Exchange: BX\u0026quot;,\rcaption = \u0026quot;Source: NASDAQ\u0026quot;,\rx = \u0026quot;Time\u0026quot;, y = \u0026quot;Price\u0026quot;,\rcolor = \u0026quot;Side\u0026quot;) +\rscale_y_continuous(labels = scales::dollar) +\rscale_color_brewer(palette = \u0026quot;Set1\u0026quot;)\rNow its up to you to do something interesting with the data, I hope RITCH can help you with it.\nSpeed \u0026amp; RAM concerns\rIf your machine struggles with some files, you can always load only parts of a file as shown above.\rAnd of course, make sure that you have only necessary datasets in your R-environment and that no unused programs are open (Chrome with some open tabs in the background happily eats your RAM).\rIf your machine is able to handle larger datasets, you can increase the buffersize of each function call to 1GB or more using the buffer_size = 1e9 argument, increasing the speed with which a file is parsed.\nAddendum\rIf you find this package useful or have any other kind of feedback, I’d be happy if you let me know.\rOtherwise, if you need more functionality for additional message types, please feel free to create an issue or a pull request on GitHub.\n","date":1512000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512000000,"objectID":"6337924d2e359a90b692823ef51cab0d","permalink":"https://davzim.github.io/blog/2017-11-30-introducing-ritch/","publishdate":"2017-11-30T00:00:00Z","relpermalink":"/blog/2017-11-30-introducing-ritch/","section":"blog","summary":"Recently I was faced with a file compressed in NASDAQ’s ITCH-protocol, as I wasn’t able to find an R-package that parses and loads the file to R for me, I spent (probably) way to much time to write one, so here it is.","tags":["R","finance","ITCH","data"],"title":"Introducing RITCH: Parsing ITCH Files in R (Finance \u0026 Market Microstructure)","type":"blog"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"7f43e90ea5c39c3de5fbcf5a3a3670dc","permalink":"https://davzim.github.io/project/optionvaluation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/optionvaluation/","section":"project","summary":"An interactive shiny application to make it easier to understand financial options","tags":["Package"],"title":"Option Valuation","type":"project"},{"authors":null,"categories":["R"],"content":"\rI recently had the opportunity to listen to some great minds in the area of high-frequency data and trading.\rWhile I won’t go into the details about what has been said, I wanted to illustrate the importance of proper out-of-sample testing and proper variable lags in potential trade algorithms or arbitrage models that has been brought up.\rThis topic can also be generalized to a certain degree to all forecasts.\rThe following example considers a case where some arbitrary trading algorithm is being tested: first without any proper tests, then with proper variable lags, and lastly using out-of-sample methodology.\rI already downloaded the data (returns for all DJIA-components from 2010 to mid-sept 2016) and uploaded it to my github-page.\rWe can load the data like this:\nlibrary(data.table)\rurl \u0026lt;- \u0026quot;https://raw.githubusercontent.com/DavZim/Out-of-Sample-Testing/master/data/djia_data.csv\u0026quot;\rdf \u0026lt;- fread(url)\rdf[, date := as.Date(date)]\r0. The “Algorithm”\rSay we have a simple algorithm that is supposed to predict the return of some stock (in this case AAPL) with some exogenous inputs (such as returns of other stocks in the Dow-Jones Industrial Average Index) using a linear model (not very realistic, but it serves the purpose for now).\rWe would take a long position if our model predicts positive returns and a short position otherwise.\rIf we control neither for out-of-sample, nor for lags, we receive a portfolio development which would look like this, pretty exciting isn’t it?!\nlibrary(ggplot2)\rlibrary(scales)\r# copy the data\rdf_none \u0026lt;- copy(df)\r# train the model\rmdl1 \u0026lt;- lm(AAPL ~ ., data = df_none[, !\u0026quot;date\u0026quot;, with = FALSE])\r# predict the values (apply the model)\rdf_none[, AAPL_fcast := predict(mdl1, newdata = df_none)]\r# calcaulate the earnings for the algorithm\rdf_none[, earnings := cumprod(1 + ifelse(sign(AAPL_fcast) \u0026gt; 0, AAPL, -AAPL))]\r# plot the data\rggplot(df_none, aes(x = date, y = earnings)) + geom_line(color = \u0026quot;#762a83\u0026quot;) + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Earnings Multiple\u0026quot;, title = \u0026quot;No Proper Tests\u0026quot;) +\rtheme_light() +\rscale_y_continuous(labels = comma)\rSomething’s fishy!\rI wouldn’t expect a trading algorithm based on a simple linear regression to turn USD 1 initial investment into USD 25,876.65 after 6 years.\rWhy does that performance seem so wrong? That is because an algorithm at the time of prediction wouldn’t have access to the data that we trained it on.\rTo simulate a more realistic situation we can use lags and out-of-sample testing.\n1. Lags\rThe key idea behind using lags is, that your trading algorithm only has access to the information it would have under realistic conditions, that is mostly, to predict the price of the next time unit \\(t + 1\\), the latest information the algorithm can possibly have is from the current time unit \\(t\\).\rFor example, if we want to predict the price for tomorrow, we can only acces information from today.\rTo backtest our strategy, we will have to make sure that the algorithm sees only the lagged exogenous variables (the returns of all the other DJIA-components).\rTo save us some time, we can also take the lead of the endogenous variable as this is equivalent to lagging all other variables (except for the date-variable in this case, but that matters only for the visualization).\n# copy and clean the data and calculate the lead\rdf_lag \u0026lt;- copy(df)\rdf_lag[, AAPL := shift(AAPL, type = \u0026quot;lead\u0026quot;)]\rdf_lag \u0026lt;- na.omit(df_lag)\r# train the model\rmdl_lag \u0026lt;- lm(AAPL ~ ., data = df_lag[, !\u0026quot;date\u0026quot;, with = FALSE])\r# predict the new values\rdf_lag[, AAPL_lead_fcst := predict(mdl_lag, newdata = df_lag)]\r# compute the earnings from the algorithm\rdf_lag[, earnings_lead := cumprod(1 + ifelse(sign(AAPL_lead_fcst) \u0026gt; 0, AAPL, -AAPL))]\r# convert into a data format that ggplot likes\rdf_plot \u0026lt;- df_lag[, .(date, AAPL_earnings = cumprod(1 + AAPL),\rearnings_lead)]\rdf_plot \u0026lt;- melt(df_plot, id.vars = \u0026quot;date\u0026quot;)\r# plot the data\rggplot(df_plot, aes(x = date, y = value, color = variable)) + geom_line() +\rlabs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Earnings Multiple\u0026quot;, title = \u0026quot;Lagged Variable\u0026quot;) +\rscale_color_manual(name = \u0026quot;Investment\u0026quot;, labels = c(\u0026quot;AAPL\u0026quot;, \u0026quot;Algorithm\u0026quot;),\rvalues = c(\u0026quot;#1b7837\u0026quot;, \u0026quot;#762a83\u0026quot;)) + theme_light()\rThat looks a bit more reasonable, though still unrealistic.\rThe algorithm outperforms the Apple stock by some magnitude of 6; an investment of USD 1 “only” gets turned into 25 (an annual growth rate of rougly 60%).\rThis unreasonable high return is due to the fact that we still haven’t conducted a proper out-of-sample test.\n2. Out-of-Sample Testing\rSo far we have trained the model (or specified if you wish) on the same dataset that we validated the quality of the algorithm on.\rOn a meta-level you can think that the information which we use to test the algorithm is already used in the model, thus the model is to a certain degree a self-fulfilling prophecy (to a certain degree, this is also related to the concept of overfitting).\rTo avoid this, we can split the dataset into two samples for training and validation.\rWe use the training dataset to train the model and the validation dataset to test the algorithm “out-of-sample”.\n# copy and clean the data\rdf_oos \u0026lt;- copy(df)\rdf_oos[, AAPL := shift(AAPL, type = \u0026quot;lead\u0026quot;)]\rdf_oos \u0026lt;- na.omit(df_oos)\r# split the data into training and validation sample\rsplitDate \u0026lt;- as.Date(\u0026quot;2015-12-31\u0026quot;)\rdf_training \u0026lt;- df_oos[date \u0026lt; splitDate]\rdf_validation \u0026lt;- df_oos[date \u0026gt;= splitDate]\r# Train the model on the training dataset\rmdl3 \u0026lt;- lm(AAPL ~ ., data = df_training[, !\u0026quot;date\u0026quot;, with = FALSE])\r# In Sample (for comparison and plotting only) - NOT the out-of-sample test\rdf_training[, AAPL_fcast := predict(mdl3, newdata = df_training)]\rdf_training[, earnings_is := cumprod(1 + ifelse(sign(AAPL_fcast) \u0026gt; 0, AAPL, -AAPL))]\r# melt into a data format that ggplot likes\rplot_df_is \u0026lt;- df_training[, .(date, AAPL = cumprod(1 + AAPL),\ralgorithm = earnings_is)]\rplot_df_is \u0026lt;- melt(plot_df_is, id.vars = \u0026quot;date\u0026quot;)\rplot_df_is[, type := \u0026quot;insample\u0026quot;]\r# Out-of Sample Test\rdf_validation[, AAPL_oos_fcast := predict(mdl3, newdata = df_validation)]\rdf_validation[, \u0026#39;:=\u0026#39; (\rearnings_oos_lead = cumprod(1 + ifelse(sign(AAPL_oos_fcast) \u0026gt; 0, AAPL, -AAPL))\r)]\r# melt into a data format that ggplot likes\rplot_df_oos \u0026lt;- df_validation[, .(date,\rAAPL = cumprod(1 + AAPL), algorithm = earnings_oos_lead)]\rplot_df_oos \u0026lt;- melt(plot_df_oos, id.vars = \u0026quot;date\u0026quot;)\rplot_df_oos[, type := \u0026quot;oos\u0026quot;]\rplot_df \u0026lt;- rbindlist(list(plot_df_is, plot_df_oos))\r# plot the data\rfacet_names \u0026lt;- c(\u0026quot;insample\u0026quot; = \u0026quot;In-Sample\u0026quot;, \u0026quot;oos\u0026quot; = \u0026quot;Out-of-Sample \u0026#39;16\u0026quot;)\rggplot(plot_df, aes(x = date, y = value, color = variable)) + geom_line() +\rlabs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Earnings Multiple\u0026quot;, title = \u0026quot;Lagged Variable + Out-of-Sample Test\u0026quot;) +\rscale_color_manual(name = \u0026quot;Investment\u0026quot;, labels = c(\u0026quot;AAPL\u0026quot;, \u0026quot;Algorithm\u0026quot;),\rvalues = c(\u0026quot;#1b7837\u0026quot;, \u0026quot;#762a83\u0026quot;)) + theme_light() + facet_wrap(~type, scales = \u0026quot;free\u0026quot;, labeller = as_labeller(facet_names))\rAs expected, the algorithm returns less than the stock it matches against (AAPL).\rIf we would invest in this simple algorithm, I would expect our investment to go down a lot more.\rAll of this is of course without considering trading, execution, and other transaction costs, which would even further decrease the returns of our trading algorithm.\rTo sum it up: If you are looking into algorithms and/or forecasts, always make sure that you apply proper out-of-sample tests and think about what information could have been used at the time of decision to avoid overfitting.\rAs always, if you find this interesting, find an error, or if you have a question you are more than welcome to leave a reply or contact me directly.\n","date":1477267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477267200,"objectID":"192e91c3a92ff2e4f6b9e03b58347390","permalink":"https://davzim.github.io/blog/2016-10-24-oos-testing/","publishdate":"2016-10-24T00:00:00Z","relpermalink":"/blog/2016-10-24-oos-testing/","section":"blog","summary":"I recently had the opportunity to listen to some great minds in the area of high-frequency data and trading.\rWhile I won’t go into the details about what has been said, I wanted to illustrate the importance of proper out-of-sample testing and proper variable lags in potential trade algorithms or arbitrage models that has been brought up.","tags":["R","finance","testing","out-of-sample testing","simulation","forecasting"],"title":"The Importance of Out-of-Sample Tests and Lags in Forecasts and Trading Algorithms","type":"blog"},{"authors":null,"categories":["R"],"content":"\rThe following entry explains a basic principle of finance, the so-called efficient frontier and thus serves as a gentle introduction into one area of finance: “portfolio theory” using R.\rA second part will then concentrate on the Capital-Asset-Pricing-Method (CAPM) and its assumptions, implications and drawbacks.\nNote: All code that is needed for the simulations, data manipulation, and plots that is not explicitly shown in this blogpost can be found on my Github page.\nIntroduction\rOne of the basic concepts of finance is the risk-return tradeoff.\rAs an illustrative example: If I were to offer you two investments with the same risk but with different expected returns, you would take the investment with the higher return (a closer look on what is meant by the terms risk and expected return follows later).\rIf the two investments had the same expected return but different levels of risk, most people would choose the investment with less risk.\rThis behaviour is called risk-aversion.\nBut what happens if you have to make the decision between a strategy of how to distribute an investment worth of $100.000 between the 30 companies listed in the the German DAX index, the 100 companies from Britain’s FTSE, the 500 companies from the S\u0026amp;P 500, or if you have the (most realistic) option to invest in all listed stocks in the world?\rHow would you distribute the cash to create a promising portfolio?\nThis is the question Markowitz tried to answer in 1952 in his paper “Portfolio Selection”.\rNow, many years later, many academics and practitioners have added their answers to the discussion, advancing the discussion, but Markowitz’s solution is still considered to be a fundamental piece of finance.\rHis portfolio selection advanced into the now called “Capital Asset Pricing Model” or short CAPM.\rThe R/Finance conference 2016 alone lists at least 4 entries that are directly linked to portfolio theory and the CAPM.\rSo what exactly is the CAPM doing, what are its implications, assumptions, what are its drawbacks, and for us most importantly, how can we leverage R to calculate it?\nLet’s create a hands-on example to illustrate the theory:\rWe begin by looking at three different, well-known stocks of the Dow-Jones Industrial Average: IBM (IBM), Google/Alphabet (GOOG), and JP Morgan (JPM).\rI have gathered monthly data since 2000 from Yahoo using the quantmod-library and saved it to my Github-page (including the script to gather the data).\rWe can load the data and plot an indexed price with the following code (notice, I use data.table for data storage and manipulation, but of course you can use dplyr, or base-r as well)\nlibrary(data.table)\rlibrary(scales)\rlibrary(ggplot2)\rlink \u0026lt;- \u0026quot;https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/fin_data.csv\u0026quot;\rdt \u0026lt;- fread(link)\rdt[, date := as.Date(date)]\r# create indexed values\rdt[, idx_price := price/price[1], by = ticker]\r# plot the indexed values\rggplot(dt, aes(x = date, y = idx_price, color = ticker)) +\rgeom_line() +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Price Developments\u0026quot;) +\rxlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Price\\n(Indexed 2000 = 1)\u0026quot;) +\rscale_color_discrete(name = \u0026quot;Company\u0026quot;)\rAltough we can see that Google outperformed the other two, we can also see that it seems it was risikier to invest in Google, as shown by the large spikes and drawdowns.\rThis is the principle of risk-return tradeoff that is fundamental to the CAPM.\rUsually, risk is defined as volatility, which is defined as the standard deviation of the returns\r.\rOn the other hand, return is either calculated using arithmetic or logarithmic returns (for simplicity I use arithmetic returns in this post, more on the mathematics of returns can be found in the appendix of this post).\rTo visualise the tradeoff, we can compute the mean and standard deviation for returns in a table or plot the two measures against each other.\n# calculate the arithmetic returns\rdt[, ret := price / shift(price, 1) - 1, by = ticker]\r# summary table\r# take only non-na values\rtab \u0026lt;- dt[!is.na(ret), .(ticker, ret)]\r# calculate the expected returns (historical mean of returns) and\r# volatility (standard deviation of returns)\rtab \u0026lt;- tab[, .(er = round(mean(ret), 4), sd = round(sd(ret), 4)), by = \u0026quot;ticker\u0026quot;]\rtab\r## ticker er sd\r## 1: IBM 0.0040 0.0554\r## 2: GOOG 0.0217 0.0801\r## 3: JPM 0.0064 0.0741\rThe table tab shows that Google had the highest expected return and also the highest volatility, whereas IBM had the lowest figures, both in terms of expected returns and volatility.\nTicker\rExpected Return\rVolatility\rIBM\r0.41%\r5.55%\rGOOG\r2.21%\r8.03%\rJPM\r0.64%\r7.43%\rOne visualisation you will have with no doubt already come across if you know a bit about the CAPM is a plot regarding this risk-return tradeoff, with the risk (volatility) on the x-axis, the expected returns on the y-axis and dots for each asset/company.\rIn our example it looks like this.\nggplot(tab, aes(x = sd, y = er, color = ticker)) +\rgeom_point(size = 5) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Risk-Return Tradeoff\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, 0.03)) +\rscale_x_continuous(label = percent, limits = c(0, 0.1))\rGiven that you had to take only a single stock and given past performance indicates future performance (more on the assumptions later, but for now: historic performances almost never indicates future performances), you would probably choose Google as your best investment choice.\rHowever, the exclusive option is not realistic as there is something called diversification and you can reduce risk by buying multiple stocks.\nComing back to the examples from the beginning: If you had the choice between three fictive investments A (expected return of 5%, volatility of 4%), B (exp ret of 5%, volatility of 5%), or C (exp ret 6%, volatility of 5%), you would prefer A over B (we say A is efficient to B) as it has less volatility, and C over B as it has a higher expected return.\rHowever, we cannot compare A and C withouth further knowledge.\rIn summary, the higher a point on the graph is to the left, the more favorable an asset is (higher expected return and less volatility).\nIf we combine assets into a new portfolio that contains stocks from two companies, our portfolio also has an expected risk-return tradeoff.\rThe crux is to calculate these values for a given set of (multiple) assets and find an optimum portfolio.\rThis is what we will do next.\rWe will start with just two assets to show the main principles, then we will add a third asset to show how to find the so-called efficient frontier, lastly we will look into different options and constraints when calculating the efficient frontier.\nCalculating the Risk-Return Tradeoff of a Portfolio\rTwo Assets\rGiven a portfolio consisting of two assets with a share of stock x of \\(\\omega_x\\) and a share of stock y of \\(\\omega_y = 1 - \\omega_x\\) (as we invest all of the cash, we get the constraint that the weights add up to one, so that \\(0 \\leq \\omega_x \\leq 1\\), \\(0 \\leq \\omega_y \\leq 1\\), and \\(\\omega_x + \\omega_y = 1\\)).\rTake an equally weighted portfolio as an example, where stock x (say Google) makes 50% of the portfolio (\\(\\omega_x = 0.5\\)) and stock y (i.e., IBM) makes up for the other 50% of the value (\\(\\omega_y = 1 - \\omega_x = 0.5\\)).\rWhat are the expected returns and volatility of the portfolio containing the two assets?\rIs it a direct linear combination of the two? Not necessarily.\rThe expected values are a function of the correlation \\(\\rho\\) between the returns of the assets.\rIf the returns are perfectly correlated (\\(\\rho = 1\\)), a portfolio would lie on this line, however, the returns are most likely not perfectly correlated.\rThe next plot shows the possible values a portfolio will take between two simulated return series given a certain correlation, the maths behind calculating this will be explained in a bit).\nWe see that the lower the correlation between two stocks is, the better it is for diversification purposes (remember usuallyy higher expected return (up) and lower volatility (left) is better).\rIn the case of a perfect negative correlation (\\(\\rho = - 1\\)) we could diversify all risk away (of course, reality has most likely no cases where two assets have a perfect positive or negative correlation).\nTo calculate the expected return \\(\\hat{r}_p\\) and the expected volatility \\(\\sigma_p\\) of a portfolio, we need to use the following formulae, wich include the weights \\(\\omega\\) for asset x and y, the expected returns \\(\\hat{r}\\), the expected risk \\(\\sigma\\) (standard deviations), as well as the covariance \\(\\sigma_{x,y}\\) (which is closely related to the correlation) for the two stocks.\nWe get the expected return for the portfolio with\n\\[\r\\hat{r}_p = \\omega_x \\hat{r}_x+ \\omega_y \\hat{r}_y\r\\]\nand the expected volatility with\n\\[\r\\sigma_p = \\sqrt{\\omega_x^2 \\sigma_x^2 + \\omega_y^2 \\sigma_y^2 + 2 \\omega_x \\omega_y \\sigma_{x,y}} \\]\nGiven that the correlation between two series can be expressed as a product of their covariance and their respective standard deviations (\\(\\sigma_{x,y} = \\rho_{x,y} \\sigma_x \\sigma_y\\)), we can see why the portfolio is relying on the correlation between the inputs.\nI simulated returns for multiple assets already (the code can be obtained here: create_multiple_asset_dataset.R).\rUsing the maths we can calculate the expected return and volatility for a portfolio for given shares.\rAs we want to see how good our portfolio is, we calculate not one, but many possible portfolios that have a share of \\(0\\leq \\omega_x \\leq 1\\) (Given that we cannot borrow money, we have the restriction \\(\\omega_x + \\omega_y = 1\\), thus we only have \\(\\omega_x\\) as a variable and not \\(\\omega_y\\)).\rFirst we have to load the returns, then we calculate the necessary values, in the last step we calculate the expected values for the portfolio.\n# load the data\rlink \u0026lt;- \u0026quot;https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/mult_assets.csv\u0026quot;\rmult_assets \u0026lt;- fread(link)\r# calculate the necessary values:\r# I) expected returns for the two assets\rer_x \u0026lt;- mean(mult_assets$x)\rer_y \u0026lt;- mean(mult_assets$y)\r# II) risk (standard deviation) as a risk measure\rsd_x \u0026lt;- sd(mult_assets$x)\rsd_y \u0026lt;- sd(mult_assets$y)\r# III) covariance\rcov_xy \u0026lt;- cov(mult_assets$x, mult_assets$y)\r# create 1000 portfolio weights (omegas)\rx_weights \u0026lt;- seq(from = 0, to = 1, length.out = 1000)\r# create a data.table that contains the weights for the two assets\rtwo_assets \u0026lt;- data.table(wx = x_weights,\rwy = 1 - x_weights)\r# calculate the expected returns and standard deviations for the # 1000 possible portfolios\rtwo_assets[, \u0026#39;:=\u0026#39; (er_p = wx * er_x + wy * er_y,\rsd_p = sqrt(wx^2 * sd_x^2 +\rwy^2 * sd_y^2 +\r2 * wx * (1 - wx) * cov_xy))]\rtwo_assets\r## wx wy er_p sd_p\r## 1: 0.000000000 1.000000000 0.03000000 0.01973497\r## 2: 0.001001001 0.998998999 0.03004001 0.01971606\r## 3: 0.002002002 0.997997998 0.03008001 0.01969728\r## 4: 0.003003003 0.996996997 0.03012002 0.01967863\r## 5: 0.004004004 0.995995996 0.03016002 0.01966010\r## --- ## 996: 0.995995996 0.004004004 0.06980549 0.04979363\r## 997: 0.996996997 0.003003003 0.06984550 0.04984333\r## 998: 0.997997998 0.002002002 0.06988550 0.04989305\r## 999: 0.998998999 0.001001001 0.06992551 0.04994277\r## 1000: 1.000000000 0.000000000 0.06996551 0.04999250\r# lastly plot the values\rggplot() +\rgeom_point(data = two_assets, aes(x = sd_p, y = er_p, color = wx)) +\rgeom_point(data = data.table(sd = c(sd_x, sd_y), mean = c(er_x, er_y)),\raes(x = sd, y = mean), color = \u0026quot;red\u0026quot;, size = 3, shape = 18) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Possible Portfolios with Two Risky Assets\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, max(two_assets$er_p) * 1.2)) +\rscale_x_continuous(label = percent, limits = c(0, max(two_assets$sd_p) * 1.2)) +\rscale_color_continuous(name = expression(omega[x]), labels = percent)\rWith the color coding we can see that the upper point stands for stock x (\\(\\omega_x = 100%\\)) and the lower point stands for stock y (\\(\\omega_x = 0%\\), thus \\(\\omega_y = 100%\\)).\rWe also see that all possible portfolios lay on a curve.\rThat means the line stands for all possible portfolios combining the two assets.\rThis will change if we add a third asset.\nAdding a Third Asset\rAdding a third asset to the portfolio expands the formula.\rFor the portfolio we get an expected return of\n\\[\r\\hat{r}_p = \\omega_x \\hat{r}_x + \\omega_y \\hat{r}_y + \\omega_z \\hat{r}_z\r\\]\rand an expected standard deviation of\n\\[\r\\sigma_p = \\sqrt{\\omega_x^2 \\sigma_x^2 + \\omega_y^2 \\sigma_y^2 + \\omega_z^2 \\sigma_z^2 +\r2 \\omega_x \\omega_y \\sigma_{x,y} + 2 \\omega_x \\omega_z \\sigma_{x,z} + 2 \\omega_y \\omega_z \\sigma_{y,z}}.\r\\]\nDoing this in R is fairly easy once you understand the last code-chunk for two assets.\rBy expanding it we get the following\n# calculate the necessary values:\r# I) expected returns for the two assets\rer_x \u0026lt;- mean(mult_assets$x)\rer_y \u0026lt;- mean(mult_assets$y)\rer_z \u0026lt;- mean(mult_assets$z)\r# II) risk (standard deviation) as a risk measure\rsd_x \u0026lt;- sd(mult_assets$x)\rsd_y \u0026lt;- sd(mult_assets$y)\rsd_z \u0026lt;- sd(mult_assets$z)\r# III) covariance\rcov_xy \u0026lt;- cov(mult_assets$x, mult_assets$y)\rcov_xz \u0026lt;- cov(mult_assets$x, mult_assets$z)\rcov_yz \u0026lt;- cov(mult_assets$y, mult_assets$z)\r# create portfolio weights (omegas)\rx_weights \u0026lt;- seq(from = 0, to = 1, length.out = 1000)\r# create a data.table that contains the weights for the three assets\rthree_assets \u0026lt;- data.table(wx = rep(x_weights, each = length(x_weights)),\rwy = rep(x_weights, length(x_weights)))\rthree_assets[, wz := 1 - wx - wy]\r# calculate the expected returns and standard deviations for the 1000 possible portfolios\rthree_assets[, \u0026#39;:=\u0026#39; (er_p = wx * er_x + wy * er_y + wz * er_z,\rsd_p = sqrt(wx^2 * sd_x^2 +\rwy^2 * sd_y^2 +\rwz^2 * sd_z^2 +\r2 * wx * wy * cov_xy +\r2 * wx * wz * cov_xz +\r2 * wy * wz * cov_yz))]\r# take out cases where we have negative weights (shortselling)\rthree_assets \u0026lt;- three_assets[wx \u0026gt;= 0 \u0026amp; wy \u0026gt;= 0 \u0026amp; wz \u0026gt;= 0] three_assets\r## wx wy wz er_p sd_p\r## 1: 0.000000 0.000000000 1.000000e+00 0.04000000 0.03005254\r## 2: 0.000000 0.001001001 9.989990e-01 0.03998999 0.03002259\r## 3: 0.000000 0.002002002 9.979980e-01 0.03997998 0.02999265\r## 4: 0.000000 0.003003003 9.969970e-01 0.03996997 0.02996272\r## 5: 0.000000 0.004004004 9.959960e-01 0.03995996 0.02993281\r## --- ## 500348: 0.996997 0.003003003 4.336809e-17 0.06984550 0.04984333\r## 500349: 0.997998 0.000000000 2.002002e-03 0.06990552 0.04989176\r## 500350: 0.997998 0.001001001 1.001001e-03 0.06989551 0.04989239\r## 500351: 0.998999 0.000000000 1.001001e-03 0.06993552 0.04994212\r## 500352: 1.000000 0.000000000 0.000000e+00 0.06996551 0.04999250\r# lastly plot the values\rggplot() +\rgeom_point(data = three_assets, aes(x = sd_p, y = er_p, color = wx - wz)) +\rgeom_point(data = data.table(sd = c(sd_x, sd_y, sd_z), mean = c(er_x, er_y, er_z)),\raes(x = sd, y = mean), color = \u0026quot;red\u0026quot;, size = 3, shape = 18) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Possible Portfolios with Three Risky Assets\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, max(three_assets$er_p) * 1.2)) +\rscale_x_continuous(label = percent, limits = c(0, max(three_assets$sd_p) * 1.2)) +\rscale_color_gradientn(colors = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;yellow\u0026quot;),\rname = expression(omega[x] - omega[z]), labels = percent)\rWe see that the area of possible portfolios has expanded into a third dimension.\rThe colors try to show the two different weights.\rA yellow color indicates a portfolio consisting mainly of the asset x, a blue color indicates asset y, and a red area indicates a portfolio of asset z.\rWe also see the three single asset-portfolios (the three red dots).\rI hope now it also becomes clear what an efficient portfolio is.\rTo give an example: We have many possible portfolios that have a volatility of 2%, but only one of them is a portfolio we would take (the one with the highest expected return) as this is more efficient than the other possible portfolios with a volatility of 2% but with less expected returns.\rTherefore we call the (upper) edge of all possible portfolios the efficient frontier.\nGenerally speaking, the expected returns and standard deviations for a portfolio consisting of n-assets are\r\\[\r\\hat{r}_p = \\sum_{i=1}^{n} \\omega_i \\hat{r}_i\r\\]\rand\r\\[\r\\sigma_p = \\sum_{i=1}^{n} \\omega_i^2 \\sigma_i^2 + \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\omega_i \\omega_j \\sigma_{i, j} \\forall i \\neq j\r\\]\nCalculating the Efficient Frontier\rThe efficient frontier can be calculated on its own without the need to simulate thousands of portfolios and then finding the efficient ones.\rWe have to distinguish two cases, one with short-selling (that is negative weights) and one without.\rWe will first look at the case without restrictions (with short-selling allowed).\rSo far we have restricted our portfolios to only contain positive weights (by filtering out negative weights).\nWith Short-Selling\rTo calculate the efficient frontier we can use the following closed-form formula that calculates the efficient frontier for a given input of risk (the input-value is thus \\(\\sigma\\)) and for some parameters (\\(\\alpha, \\beta, \\gamma, \\delta\\)) using matrix algebra (note, I put the formula here if someone wants to calculate it by hand; if you are just interested in the R code, you can skip the short mathematics section).\n\\[\r\\hat{r}_{ef}(\\sigma) = \\frac{\\beta}{\\alpha} + \\sqrt{\\left(\\frac{\\beta}{\\alpha}\\right)^2 - \\frac{\\gamma - \\delta * \\sigma^2}{\\alpha}},\r\\]\nwhich is the solution to a quadratic optimization problem.\rThe parameters are given by the following matrix algebra\n\\[\r\\alpha = 1^T s^{-1} 1,\r\\]\nwhere \\(1\\) is a matrix of 1’s with a length of the numbers of stocks, \\(s\\) is a matrix of the covariances between the assets (with a dimension of n times n).\n\\[\r\\beta = 1^T s^{-1} \\overline{ret},\r\\]\nwhere \\(\\overline{ret}\\) stands for a vector of average returns for each stock.\n\\[\r\\gamma = \\overline{ret}^T s^{-1} \\overline{ret},\r\\]\nand lastly \\(\\delta\\) is given by\n\\[\r\\delta = \\alpha \\gamma - \\beta ^2\r\\]\nAs you can see, the only inputs are the returns of stocks, in R we can write a short function (calcEFParams) that calculates the parameters and returns a list with the parameters.\ncalcEFParams \u0026lt;- function(rets) {\rretbar \u0026lt;- colMeans(rets, na.rm = T)\rcovs \u0026lt;- var(rets, na.rm = T) # calculates the covariance of the returns\rinvS \u0026lt;- solve(covs)\ri \u0026lt;- matrix(1, nrow = length(retbar))\ralpha \u0026lt;- t(i) %*% invS %*% i\rbeta \u0026lt;- t(i) %*% invS %*% retbar\rgamma \u0026lt;- t(retbar) %*% invS %*% retbar\rdelta \u0026lt;- alpha * gamma - beta * beta\rretlist \u0026lt;- list(alpha = as.numeric(alpha),\rbeta = as.numeric(beta),\rgamma = as.numeric(gamma),\rdelta = as.numeric(delta))\rreturn(retlist)\r}\rabcds \u0026lt;- calcEFParams(mult_assets)\rabcds\r## $alpha\r## [1] 4037.551\r## ## $beta\r## [1] 147.8334\r## ## $gamma\r## [1] 5.992395\r## ## $delta\r## [1] 2339.881\rcalcEFValues \u0026lt;- function(x, abcd, upper = T) {\ralpha \u0026lt;- abcd$alpha\rbeta \u0026lt;- abcd$beta\rgamma \u0026lt;- abcd$gamma\rdelta \u0026lt;- abcd$delta\rif (upper) {\rretval \u0026lt;- beta / alpha + sqrt((beta / alpha) ^ 2 - (gamma - delta * x ^ 2) / (alpha))\r} else {\rretval \u0026lt;- beta / alpha - sqrt((beta / alpha) ^ 2 - (gamma - delta * x ^ 2) / (alpha))\r}\rreturn(retval)\r}\r# calculate the risk-return tradeoff the two assets (for plotting the points)\rdf_table \u0026lt;- melt(mult_assets)[, .(er = mean(value),\rsd = sd(value)), by = variable]\r# plot the values\rggplot(df_table, aes(x = sd, y = er)) +\r# add the stocks\rgeom_point(size = 4, color = \u0026quot;red\u0026quot;, shape = 18) +\r# add the upper efficient frontier\rstat_function(fun = calcEFValues, args = list(abcd = abcds, upper = T), n = 10000,\rcolor = \u0026quot;red\u0026quot;, size = 1) +\r# add the lower \u0026quot;efficient\u0026quot; frontier\rstat_function(fun = calcEFValues, args = list(abcd = abcds, upper = F), n = 10000,\rcolor = \u0026quot;blue\u0026quot;, size = 1) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Efficient Frontier with Short-Selling\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +\rscale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2))\rGiven the values for \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) we receive the function for the frontier as\n\\[\r\\hat{r}_{ef}(\\sigma) = \\frac{147.8}{4037.6} \\pm \\sqrt{\\left(\\frac{147.8}{4037.6}\\right)^2 - \\frac{6.0 - 2339.9 * \\sigma^2}{4037.6}}.\r\\]\nThe values for the +-part of the function is the upper, efficient frontier, whereas the --part represents the lower, inefficient frontier.\nThe red curve (the upper curve) is the real efficient frontier, whereas the lower (blue) curve stands for an inefficient frontier.\rThis is due to the fact, that we can create a mixture of the three assets that has the same volatility but a higher expected return.\rAs we are able to shortsell (borrow money by selling stocks that we don’t own and investing this cash) the efficient frontier doesn’t necessarily touch the three assets, nor does it end at the points but extends outwards.\nAlthough the maths is not trivial (at least when it comes to understanding), the calculation of the efficient frontier is given by a closed mathematical function and its calculation is straightforward.\rThis will change in the next chapter, when we add the restrictions that all weights have to be larger than zero (i.e., no short-selling).\nWithout Short-Selling\rRestricting the portfolio selection by only having positive weights of the assets limits the amounts of possible portfolios and introduces complexity that cannot be handled by closed-form mathematics, thus we need to fall back to mathematical optimization.\rThere is a wide variety of possible packages that are able to do this, I found the portfolio.optim-function of the tseries-package most useful.\nThe function takes asset returns, and a desired portfolio return as an input (besides other parameters that we do not use here) and returns information about the portfolio at that desired return.\rWe can write a short wrapper function that gives us the efficient frontier.\nlibrary(tseries)\rdf_table \u0026lt;- melt(mult_assets)[, .(er = mean(value),\rsd = sd(value)), by = variable]\rer_vals \u0026lt;- seq(from = min(df_table$er), to = max(df_table$er),\rlength.out = 1000)\r# find an optimal portfolio for each possible possible expected return # (note that the values are explicitly set between the minimum and maximum of\r# the expected returns per asset)\rsd_vals \u0026lt;- sapply(er_vals, function(er) {\rop \u0026lt;- try(portfolio.optim(as.matrix(mult_assets), er), silent = TRUE)\rif (inherits(op, \u0026quot;try-error\u0026quot;)) return(NA)\rreturn(op$ps)\r})\rplot_dt \u0026lt;- data.table(sd = sd_vals, er = er_vals)\rplot_dt \u0026lt;- plot_dt[!is.na(sd)]\r# find the lower and the upper frontier\rminsd \u0026lt;- min(plot_dt$sd)\rminsd_er \u0026lt;- plot_dt[sd == minsd, er]\rplot_dt[, efficient := er \u0026gt;= minsd_er]\rplot_dt\r## sd er efficient\r## 1: 0.01965715 0.03004001 FALSE\r## 2: 0.01958006 0.03008001 FALSE\r## 3: 0.01950373 0.03012002 FALSE\r## 4: 0.01942815 0.03016002 FALSE\r## 5: 0.01935335 0.03020003 FALSE\r## --- ## 994: 0.04965690 0.06976548 TRUE\r## 995: 0.04972396 0.06980549 TRUE\r## 996: 0.04979104 0.06984550 TRUE\r## 997: 0.04985816 0.06988550 TRUE\r## 998: 0.04992531 0.06992551 TRUE\rggplot() +\rgeom_point(data = plot_dt[efficient == F], aes(x = sd, y = er), size = 0.5, color = \u0026quot;blue\u0026quot;) +\rgeom_point(data = plot_dt[efficient == T], aes(x = sd, y = er), size = 0.5, color = \u0026quot;red\u0026quot;) +\rgeom_point(data = df_table, aes(x = sd, y = er), size = 4, color = \u0026quot;red\u0026quot;, shape = 18) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Efficient Frontier without Short-Selling\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +\rscale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2))\rComparing the two scenarios we see that restricting the weights (prohibiting short-sells), we also restrict the number of possible portfolios.\rIn some cases, restricting short-sells yields portfolios with less expected returns or a higher expected volatility.\nTo directly compare the two options we can use the following code.\n# combine the data into one plotting data.table called \u0026quot;pdat\u0026quot;\r# use plot_dt with constraints\rpdat1 \u0026lt;- plot_dt[, .(sd, er, type = \u0026quot;wo_short\u0026quot;, efficient)]\r# calculate the values without constraints\rpdat2lower \u0026lt;- data.table(sd = seq(from = 0, to = max(pdat1$sd) * 1.2, length.out = 1000))\rpdat2lower[, \u0026#39;:=\u0026#39; (er = calcEFValues(sd, abcds, F),\rtype = \u0026quot;short\u0026quot;,\refficient = F)]\rpdat2upper \u0026lt;- data.table(sd = seq(from = 0, to = max(pdat1$sd) * 1.2, length.out = 1000))\rpdat2upper[, \u0026#39;:=\u0026#39; (er = calcEFValues(sd, abcds, T),\rtype = \u0026quot;short\u0026quot;,\refficient = T)]\rpdat \u0026lt;- rbindlist(list(pdat1, pdat2upper, pdat2lower))\r# plot the values\rggplot() +\rgeom_line(data = pdat, aes(x = sd, y = er, color = type, linetype = efficient), size = 1) +\rgeom_point(data = df_table, aes(x = sd, y = er), size = 4, color = \u0026quot;red\u0026quot;, shape = 18) +\r# Miscellaneous Formatting\rtheme_bw() + ggtitle(\u0026quot;Efficient Frontiers\u0026quot;) +\rxlab(\u0026quot;Volatility\u0026quot;) + ylab(\u0026quot;Expected Returns\u0026quot;) +\rscale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +\rscale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2)) +\rscale_color_manual(name = \u0026quot;Short-Sells\u0026quot;, values = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), labels = c(\u0026quot;Allowed\u0026quot;, \u0026quot;Prohibited\u0026quot;)) +\rscale_linetype_manual(name = \u0026quot;Efficient\u0026quot;, values = c(2, 1))\rThere are many ways to do this and I am certainly not claiming that this is the only option for portfolio selection, but the efficient frontier is a very useful tool to understand one of the basics of finance.\rThe question of which portfolio of the efficient frontier is considered best within the model-frameworks, has to be left open in this part, but will be adressed using CAPM in part II.\nThis closes the first part of the series.\rThe next section will look more into the CAPM, which is very closely related to the efficient frontier (many would argue that the efficient frontier is part of the CAPM) and its implications, assumptions, and drawbacks.\rIn the meantime, if you have questions, feedback or issues, please leave a comment or write me an email (you find my email in the about-section).\nLastly, if you want to use the graphics or code for your own teaching, please contact me and make sure that you give credit and link back to my blog.\rThank you very much.\rAlso as a disclaimer, I am in no way giving any financial advice in this post.\nAppendix\rOne of the many issues I haven’t properly addressed in this entry is the calculation of returns.\rThere are a couple of ways of how to calculate the returns, most often used are arithmetic and logarithmic returns.\nArithmetic Returns\r\\[\rr_t = \\frac{P_t - P_{t-1}}{P_{t-1}} = \\frac{P_t}{P_{t-1}} - 1\r\\]\rLogarithmic Returns\r\\[\rr_t = log\\left(\\frac{P_t}{P_{t-1}}\\right) = log(P_t) - log(P_{t-1})\r\\]\nThe difference between the two is enough to cover another blog post, which is what Pat of PortfolioProbe already did, so if you want to know more about the two, check out his “Tale of Two Returns”.\n","date":1464048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464048000,"objectID":"42271dbd1422ef2e88aacbec84080d9d","permalink":"https://davzim.github.io/blog/2016-05-24-gentle-capm-i/","publishdate":"2016-05-24T00:00:00Z","relpermalink":"/blog/2016-05-24-gentle-capm-i/","section":"blog","summary":"The following entry explains a basic principle of finance, the so-called efficient frontier and thus serves as a gentle introduction into one area of finance: “portfolio theory” using R.\rA second part will then concentrate on the Capital-Asset-Pricing-Method (CAPM) and its assumptions, implications and drawbacks.","tags":["R","finance","capm","efficient-frontier"],"title":"A Gentle Introduction to Finance using R: Efficient Frontier and CAPM - Part 1","type":"blog"},{"authors":null,"categories":["R"],"content":"\rThis is a direct (though minor) answer to Daniel’s blogpost http://daniellakens.blogspot.de/2016/01/power-analysis-for-default-bayesian-t.html, which I found very interesting, as I have been trying to get my head around Bayesian statistics for quite a while now. However, one thing that bugs me, is the time needed for the simulation. On my machine it took around 22 minutes. Depending on the task, 22 minutes for a signle test can be way too long (especially if the tests are done in a business environment where many tests are needed - yesterday) and a simple t-test might be more appealing only because it takes a shorter computing time. Here is my solution to speed-up the code using snowfall’s load-balancing parallel structures to reduce the time to 8.5 minutes.\nThe initial code (which you can find in the original post) uses a for loop, snowfall, however, uses a function that we need to export, which is simply called simFun. The function takes the sample size n, the true effect size D, as well as the effect size of the alternative hypothesis rscaleBF as arguments (note, the names are equivalent to Daniel’s, as well as the values of the variables (i.e., n = 50 etc.)).\nsimFun \u0026lt;- function(n, D, rscaleBF){\rlibrary(BayesFactor)\rx \u0026lt;- rnorm(n = n, mean = 0, sd = 1)\ry \u0026lt;- rnorm(n = n, mean = D, sd = 1)\rreturn(exp((ttestBF(x, y, rscale = rscaleBF))@bayesFactor$bf))\r}\rTo load, initiate, and finally execute the snowfall-code, we can use the following:\n# load the library\rlibrary(snowfall)\rn \u0026lt;- 50; D \u0026lt;- 0.0; nSim \u0026lt;- 100000; rscaleBF \u0026lt;- sqrt(2)/2; threshold \u0026lt;- 3\r# for time-keeping\rt0 \u0026lt;- Sys.time()\r# initiate a parallel cluster with 4 cpus\rsfInit(parallel = T, cpus = 4)\r# export the function to the clusters\rsfExport(\u0026quot;simFun\u0026quot;, \u0026quot;n\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;rscaleBF\u0026quot;)\r# execute the code\rbf \u0026lt;- sfClusterApplyLB(1:nSim, function(i) simFun(n = n, D = D, rscaleBF = rscaleBF))\r# stop the clusters\rsfStop()\r# print the time it took for the calculation\rSys.time() - t0\r# Time difference of 8.490639 mins\r# and finally the result\rsum(bf \u0026lt; (1/threshold))/nSim\r#[1] 0.68578\rWith Daniel’s code I got supportH0 = 0.6904, the snowfall-code almost reproduces the same number 0.68578, but reduces the time substantially.\nI hope you found this post interesting/helpful, should you have any questions, you are more then welcome to ask them here, otherwise send me an e-mail.\nLastly, thanks to Daniel, who was able to find some bugs in an earlier version of the code.\nNote, the earlier version stated that the code was improved by 60x (without giving a result), the numbers are now corrected.\n","date":1452816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452816000,"objectID":"738f348dc9713d5aeec0d02729885c4b","permalink":"https://davzim.github.io/blog/2016-01-15-speeding-bayesian-t-test/","publishdate":"2016-01-15T00:00:00Z","relpermalink":"/blog/2016-01-15-speeding-bayesian-t-test/","section":"blog","summary":"This is a direct (though minor) answer to Daniel’s blogpost http://daniellakens.blogspot.de/2016/01/power-analysis-for-default-bayesian-t.html, which I found very interesting, as I have been trying to get my head around Bayesian statistics for quite a while now.","tags":["R","monte-carlo","snowfall","parallel"],"title":"Speeding \"Bayesian Power Analysis t-test\" up with Snowfall","type":"blog"},{"authors":null,"categories":["R"],"content":"\rYou could say that the following post is an answer/comment/addition to Quintuitive, though I would consider it as a small introduction to parallel computing with snowfall using the thoughts of Quintuitive as an example.\rA quick recap: Say you create a model that is able to forecast 60% of market directions (that is, in 6 out of 10 cases you can predict the direction of the market or an asset for a respective time-period), how well would you do using this strategy?\rHint: if you have a model that is able to predict 60% out-of-sample (think test dataset instead of training dataset) please make sure to share the strategy with me! :)\rThere are multiple ways to do it, I will show you how to simulate multiple cases using real-life financial data from the German Dax index, Monte-Carlo techniques, and parallel computing using the snowfall-package of the R language.\nThe piece is structured as follows:\nLoad financial data using quantmod\rShow one simulation case with a probability of 51%\rSimulate n cases for one probability and average the result\rSimulate k different cases with different probabilities\rUse the snowfall-package to use parallel techniques\r1. Load Financial Data using Quantmod\rTo have some data we look at the German Dax index (^GDAXI), which consists of the 30 largest companies in Germany. We can load the data and transform it to a dataframe like this:\n# load packages\rlibrary(quantmod)\r# download DAX data from Yahoo\rdax \u0026lt;- getSymbols(\u0026quot;^GDAXI\u0026quot;, from = \u0026quot;2000-01-01\u0026quot;, auto.assign = F)\r# create a data.frame Sdata, that contains the stock data\rSdata \u0026lt;- data.frame(date = index(dax), price = as.numeric(Ad(dax)))\r# calculate returns\rSdata$rets \u0026lt;- Sdata$price / c(NA, Sdata$price[1:(nrow(Sdata) - 1)]) - 1\rhead(Sdata)\r# date price rets\r# 1 2010-01-04 6048.30 NA\r# 2 2010-01-05 6031.86 -0.0027181096\r# 3 2010-01-06 6034.33 0.0004095279\r# 4 2010-01-07 6019.36 -0.0024808413\r# 5 2010-01-08 6037.61 0.0030318839\r# 6 2010-01-11 6040.50 0.0004786889\r# create a first plot to have a look at the price development\rplot(x = Sdata$date, y = Sdata$price, type = \u0026quot;l\u0026quot;, main = \u0026quot;Dax Development\u0026quot;)\r2. Show one simulation case with a probability of 51%\rNow that we have some data, we create a function get.fprice that takes in three arguments: the returns of an asset, the percentage of right predictions, and an initial price of the investment (or just the first price of the benchmark).\rThe function returns a vector of values of the investment.\nget.fprice \u0026lt;- function(rets, perc.right, init.price){\r# 1. sample the goodness of the returns\rgood.forecast \u0026lt;- sample(x = c(T, F),\rsize = length(rets),\rprob = c(perc.right, 1 - perc.right),\rreplace = T)\r# 2. get the forecasted directions, the same as the true rets\r# if good.forecast = T\rdir \u0026lt;- ifelse(rets \u0026gt; 0, 1, -1)\rforecast.dir \u0026lt;- ifelse(good.forecast, dir, -dir)\r# if the percentage sampled should be displayed\r# mean(dir == forecast.dir, na.rm = T) # 3. calculate the return of the forecast\rforecast.ret \u0026lt;- forecast.dir * rets\r# 4. calculate the prices\rshift.forecast.ret \u0026lt;- forecast.ret[2:length(forecast.ret)]\rforecast.price \u0026lt;- cumprod(1 + shift.forecast.ret) * init.price\rforecast.price \u0026lt;- c(init.price, forecast.price)\rreturn(forecast.price)\r}\rWith this function we are able to create a single simulation for one probability.\n# set a seed for reproducability\rset.seed(42)\r# simulate one series of prices\rSdata$fprice \u0026lt;- get.fprice(rets = Sdata$rets, perc.right = 0.51,\rinit.price = Sdata$price[1])\r# plot the two developments\rplot(x = Sdata$date, y = Sdata$price, type = \u0026quot;l\u0026quot;, main = \u0026quot;Dax vs. Forecast\u0026quot;, xlab = \u0026quot;Date\u0026quot;, ylab = \u0026quot;Price\u0026quot;,\rylim = c(0, max(Sdata$price, Sdata$fprice)))\rlines(x = Sdata$date, y = Sdata$fprice, col = \u0026quot;red\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Dax\u0026quot;, paste(perc.right, \u0026quot;forecast\u0026quot;)), col = 1:2, lty = 1)\rIf you play around with the seed, you will see that not all cases show a similar picture.\rThis is something inherent to simulations.\rTo avoid being overly dependent on the seed value for the random number generation, we use the law of large numbers and simulate not one but multiple cases and use the average as a result.\n3. Simulate n cases for one probability and average the result\rMonte-Carlo based simulations are multiple simulation of random developments.\rWith the next code-snippet we can simulate n cases per probability (i.e., we take n simulated paths and average them).\rFirst we create a function get.fprice.n that works like the get.fprice-function, but creates not one but n = 10000 cases using the apply-function family.\nget.fprice.n \u0026lt;- function(rets, perc.right, init.price, n){\r# create a function that produces the goodness of the forecast\rget.good.forecast \u0026lt;- function(x){\rgood.forecast \u0026lt;- sample(x = c(T, F),\rsize = length(rets),\rprob = c(perc.right, 1 - perc.right),\rreplace = T)\rreturn(good.forecast)\r}\r# 1. sample the goodness of the returns\rgood.forecasts \u0026lt;- sapply(1:n, get.good.forecast)\r# 2. get the forecasted directions, the same as the true rets # if good.forecast = T\rdir \u0026lt;- ifelse(rets \u0026gt; 0, 1, -1)\rforecast.dirs \u0026lt;- apply(good.forecasts, 2, function(x) {\rifelse(x, dir, -dir)\r})\r# 3. calculate the return of the forecast\rforecast.rets \u0026lt;- forecast.dirs * rets\r# 4. calculate the prices\rforecast.prices \u0026lt;- apply(forecast.rets, 2, function(x) {\rcumprod(1 + x[2:length(x)]) * init.price\r})\rforecast.prices \u0026lt;- rbind(rep(init.price, ncol(forecast.prices)),\rforecast.prices)\r# collapse the n simulations to just one by taking the average\rforecast.price \u0026lt;- apply(forecast.prices, 1, mean)\rreturn(forecast.price)\r}\r# simulate 10.000 cases # set a seed for reproducability, # should not matter due to the Law Of Large Numbers\rset.seed(42)\r# simulate 10.000 series of prices\rt \u0026lt;- Sys.time()\rSdata$fprice \u0026lt;- get.fprice.n(rets = Sdata$rets, perc.right = 0.51,\rinit.price = Sdata$price[1],\rn = 10000)\rSys.time() - t # takes 5.69257 seconds on my machine\r# plot the two developments\rplot(x = Sdata$date, y = Sdata$price, type = \u0026quot;l\u0026quot;, main = \u0026quot;Dax vs. Forecasts\u0026quot;, xlab = \u0026quot;Date\u0026quot;, ylab = \u0026quot;Price\u0026quot;,\rylim = c(0, max(Sdata$price, Sdata$fprice)))\rlines(x = Sdata$date, y = Sdata$fprice, col = \u0026quot;red\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Dax\u0026quot;, \u0026quot;aggregated forecasts\u0026quot;), col = 1:2, lty = 1)\r4. Simulate k different cases with different probabilities\rYou may want to compare not just one case with 51% but maybe 2, 5 or 100 cases (k) if the time allows it.\rThe following section creates k = 4 cases that range from 45% to 55% of directions predicted correctly.\rEach case is simulated n = 10000 times and then averaged.\nk \u0026lt;- 4\r# the percentages that will be used later on\rperc.right \u0026lt;- seq(from = 0.45, to = 0.55, length.out = k)\rperc.right\r#\u0026gt; [1] 0.4500000 0.4833333 0.5166667 0.5500000\r# simulate k cases n times, equals 40.000 times\rt \u0026lt;- Sys.time()\rforecasted.prices \u0026lt;- sapply(perc.right, function(x) {\rget.fprice.n(rets = Sdata$rets, perc.right = x,\rinit.price = Sdata$price[1],\rn = 10000)\r})\rSys.time() - t # takes 21.592 seconds on my machine\r# plot the results\rplot(x = Sdata$date, y = Sdata$price, type = \u0026quot;l\u0026quot;, main = \u0026quot;Dax vs. Forecasts\u0026quot;, xlab = \u0026quot;Date\u0026quot;, ylab = \u0026quot;Price\u0026quot;,\rylim = c(0, max(forecasted.prices, Sdata$price)))\rfor (i in 1:k){\rlines(x = Sdata$date, y = forecasted.prices[, i], col = (i + 1))\r}\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Dax\u0026quot;, paste0(\u0026quot;P = \u0026quot;, round(perc.right, 2), sep = \u0026quot;)), col = 1:(k + 1), lty = 1, cex = 0.75)\r5. Use the snowfall-package to use parallel techniques\rNow to the interesting part - using parallel techniques to reduce the computation time (time to beat: ~21 seconds).\rThere are many packages out there in the “R-osphere”.\rI found the snowfall package most useful and easiest to use.\rIf you want to get a broader picture of the possibilities you should head over to Ryan’s slides, which give a comprehensive analysis of high-performance computing in R.\nMy simplified understanding of parallel computing so far is that there is one master node (think of it as a departement head that coordinates the workers) and multiple slave nodes (the workers).\rThe master node then distributes tasks to each slave that computes the result and sends it back to the master.\nIn snowfall we first have to specify the number of slaves by initiating the clusters with the sfInit command.\rAs each slave starts a new environment, we have to export the libraries, functions, and datasets that the slaves will use by calling the sfExport-function (alternative we can call sfExportAll, which exports all elements in the current environment).\rSnowfall has it’s own set of functions that work very similar to the apply-family, which we call to hand over the computation to the master node.\rTo be more precise we call sfClusterApplyLB (LB stands for load balancing).\rAs a last step we have to stop the cluster by calling sfStop.\rEasy, right?\nThe full snowfall code looks like this:\n# load the library\rlibrary(snowfall)\r# initiate the data\rk \u0026lt;- 4\rperc.right \u0026lt;- seq(from = 0.45, to = 0.55, length.out = k)\r# initiate the clusters, in this case 4 slaves\rsfInit(parallel = T, cpus = 4)\r# if you are unsure how many cpus you can use, try\r# ?parallel::detectCores\r# parallel::detectCores(logical = T)\r# export the necessary data and functions to each cluster\rsfExport(\u0026quot;get.fprice.n\u0026quot;, \u0026quot;Sdata\u0026quot;)\rt \u0026lt;- Sys.time()\r# use the snowfall cluster-apply, which works similar to sapply # note that I used sfClusterApplyLB and not sfClusterApply # the LB stands for Load-Balancing\rresult \u0026lt;- sfClusterApplyLB(perc.right, function(x){\rget.fprice.n(rets = Sdata$rets, perc.right = x,\rinit.price = Sdata$price[1],\rn = 10000)\r})\rSys.time() - t #\u0026gt; 9.861 seconds\r# IMPORTANT: Stop the cluster so it doesn\u0026#39;t messes around in the background\rsfStop()\r# have a look at the data\rstr(result)\r#\u0026gt; List of 4\r#\u0026gt; $ : num [1:1463] 6048 6047 6046 6045 6043 ...\r#\u0026gt; $ : num [1:1463] 6048 6048 6048 6047 6047 ...\r#\u0026gt; $ : num [1:1463] 6048 6049 6049 6049 6050 ...\r#\u0026gt; $ : num [1:1463] 6048 6050 6050 6052 6053 ...\r# convert to a data.frame\rforecasted.prices \u0026lt;- data.frame(matrix(unlist(result), ncol = length(result)))\rhead(forecasted.prices)\r#\u0026gt; X1 X2 X3 X4\r#\u0026gt; 1 6048.300 6048.300 6048.300 6048.300\r#\u0026gt; 2 6046.659 6047.672 6048.888 6050.085\r#\u0026gt; 3 6046.424 6047.589 6048.989 6050.323\r#\u0026gt; 4 6045.218 6047.276 6049.403 6051.897\r#\u0026gt; 5 6043.136 6046.546 6049.886 6053.465\r#\u0026gt; 6 6042.890 6046.399 6049.992 6053.763\r# and finally the last plot\rplot(x = Sdata$date, y = Sdata$price, type = \u0026quot;l\u0026quot;, main = \u0026quot;Snowfall Forecasts\u0026quot;, xlab = \u0026quot;Date\u0026quot;, ylab = \u0026quot;Price\u0026quot;,\rylim = c(0, max(forecasted.prices, Sdata$price)))\rfor (i in 1:k){\rlines(x = Sdata$date, y = forecasted.prices[, i], col = (i + 1))\r}\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Dax\u0026quot;, paste0(\u0026quot;P = \u0026quot;, round(perc.right, 2), sep = \u0026quot;)), col = 1:(k + 1), lty = 1, cex = 0.75)\rThe plot shows basically the same picture compared to the last picture.\rEverything else would be very strange.\nUsing parallel techniques, we can reduce the time for the computations from 21 seconds to 9 seconds.\rIf we increase the number of cases (n = 100,000 instead of 10,000) then we should see that the parallel computation takes only a quarter of the time with 4 clusters.\nSummary\rA few lines of code shows us that even a small advantage (being able to forecast 5%p more correct Dax movements than a coin) can lead to monstrous returns of roughly 30% p.a. (CAGR of an initial investment of 6,050 and a value of 23,480 after 5.5 years).\rHowever, the take-away should be that if you find such a model, make sure that you backtest the hell out of it before telling anyone about it.\rEx post, everyone is a billionaire.\rAnother take-away and the purpose of this post is how to use Monte-Carlo and parallel computing techniques to create a simple and fast simulation to check something.\nOutro\rI am not an expert in parallel computing, so should you find an error in the explanation or in the code, please leave a comment and I will happily correct the error.\rFurthermore, if you have any questions, comments or ideas, please leave a comment or send me an email.\n","date":1442966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442966400,"objectID":"575eee25c2492c3ba004dfb255bcd757","permalink":"https://davzim.github.io/blog/2015-09-23-backtests-stock-returns/","publishdate":"2015-09-23T00:00:00Z","relpermalink":"/blog/2015-09-23-backtests-stock-returns/","section":"blog","summary":"You could say that the following post is an answer/comment/addition to Quintuitive, though I would consider it as a small introduction to parallel computing with snowfall using the thoughts of Quintuitive as an example.","tags":["R","finance","monte-carlo","quantmod","parallel","simulation","snowfall"],"title":"Simulating backtests of stock returns using Monte-Carlo and snowfall in parallel","type":"blog"},{"authors":null,"categories":["R"],"content":"\rFacing a simple, yet frustrating formula like this\n\\[xe^{ax}=b\\]\nand the task to solve it for x left me googling around for hours until I found salvation in Wolfram Alpha, Wikipedia, and a nice blogpost with R-syntax to solve a similar equation.\nUsing the results from Wolfram Alpha I was able to find the solution with the gsl library\n# install.packages(\u0026quot;gsl\u0026quot;)\rlibrary(gsl)\r# create some example data\rdat \u0026lt;- data.frame(a = 0.109861, x = 10)\r# a is set so that b is roughly 30. # Lazy as I am I used Excel and its solver ability to find numbers\r# to check if b is close to 30. Using the initial formula\rdat$b \u0026lt;- dat$x * exp(dat$a * dat$x)\rdat\r# solve for x2 and see if x and x2 are similar and close to 10\rdat$x2 \u0026lt;- lambert_W0(dat$a * dat$b)/dat$a\rdat\r#\u0026gt; a x b2 x2\r#\u0026gt;1 0.109861 10 29.99993 10.00001\rHurray!\nSometimes life can be so easy (after a long time searching for the right results….).\nAppendix: Improvements\rAfter revisiting this article some time later, I wondered what the speed is compared to Dan Kelley’s (see comment below) alternative. After firing up some repetitions using microbenchmark I got the following:\nlibrary(gsl)\rlibrary(rootSolve)\rlibrary(microbenchmark)\rlibrary(ggplot2)\rdat \u0026lt;- data.frame(a = 0.109861, x = 10)\rdat$b \u0026lt;- dat$x * exp(dat$a * dat$x)\rf \u0026lt;- function(x, a, b) x*exp(a*x) - b\rautoplot(microbenchmark(\rlambertW = dat$x2 \u0026lt;- lambert_W0(dat$a * dat$b)/dat$a,\runiroot = dat$x3 \u0026lt;- uniroot.all(f, interval = c(0, 100),\ra = dat$a, b = dat$b),\rtimes = 10000))\r","date":1437004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437004800,"objectID":"10666f02abab6f3e70de8f0ede9be0f3","permalink":"https://davzim.github.io/blog/2015-07-16-lamberts-w/","publishdate":"2015-07-16T00:00:00Z","relpermalink":"/blog/2015-07-16-lamberts-w/","section":"blog","summary":"Facing a simple, yet frustrating formula like this\n\\[xe^{ax}=b\\]\nand the task to solve it for x left me googling around for hours until I found salvation in Wolfram Alpha, Wikipedia, and a nice blogpost with R-syntax to solve a similar equation.","tags":["R","math"],"title":"Getting that X with the Glog function and Lambert's W","type":"blog"},{"authors":null,"categories":["R"],"content":"\rWhen dealing with large datasets that potentially exceed the memory of your machine it is nice to have another possibility such as your own server with an SQL/PostgreSQL database on it, where you can query the data in smaller digestible chunks. For example, recently I was facing a financial dataset of 5 GB. Although 5 GB fit into my RAM the data uses a lot of resources. One solution is to use an SQL-based database, where I can query data in smaller chunks, leaving resources for the computation.\nWhile MySQL is the more widely used, PostgreSQL has the advantage of being open source and free for all usages. However, we still need to get a server. One possible way to do it is to rent Amazon server, however, as I don’t have a budget for my projects and because I only need the data on my own machine I wanted to set up a server on my Windows 8.1 machine. This is how I did it.\nInstalling software, Starting the Server and Setting up new Users\rFirst, we need to install the necessary software. Besides R and RStudio we need to install PostgreSQL, that we find PostgreSQL. When installing we are asked to provide a password, just remember it as we need it later. Say for this example we set the password to: \u0026quot;DataScienceRocks\u0026quot;.\nNow we can already access and use the database, for example we can start the interface (pgAdmin III) that was automatically installed with PostgreSQL. To connect to the database double click on the server in pgAdmin III and type in your password. The server seems to run after the installation as well. If this is not the case (i.e. you get the error “Server doesn’t listen” when trying to connect to the server with pgAdmin III), you can start the server with the following command in the command line:\npg_ctl -D \u0026quot;C:\\Program Files\\PostgreSQL\\9.4\\data\u0026quot; start\rAs we can see, we only have one user (“postgres”). It is good practice to use the database with another user that has no createrole (think of it as a non-admin user).\nTo set up a new user I follow PostgreSQL Server installation and configuration explanation. Start the command line (go to the start menu and type cmd\u0026quot;) and move to the folder where you installed PostgreSQL (more precisely, the bin-folder). In my case I navigated to the folder by typing:\ncd C:/Program Files/PostgreSQL/9.4/bin\rNow we need to create a new user (openpg), which we can do by executing the following command:\ncreateuser.exe --createdb --username postgres --no-createrole --pwprompt openpg\rWe have to enter the password for the new user twice (note that there is no feedback from the commandline), for this example I set it to “new_user_password”, lastly we are asked to give our password for the main user (“postgres”) which is in this case “DataScienceRocks”, as specified during the installation.\nWe can check if we have two users by going into pgAdmin III, which should look like this:\nCreating a Table in pgAdmin III\rAn easy way to create a table (database) is to use pgAdmin III. Right click on the “Tables” and choose “New Table”.\nFor this example we create a table called cartable that we will later populate with the dataset of mtcars. For the dataset we need to specify the columes and their types as shown in the next picture.\nLastly, we need to specificy a primary key in constraints. In this case I use the carname column as key.\nConnection with R\rNow it is time to connect to the database with R. This approach uses the RPostgreSQL and this R and PostgreSQL – using RPostgreSQL and sqldf.\nTo connect, we need to enter the following commands in R:\n# install.packages(\u0026quot;RPostgreSQL\u0026quot;)\rrequire(\u0026quot;RPostgreSQL\u0026quot;)\r# create a connection\r# save the password that we can \u0026quot;hide\u0026quot; it as best as we can by collapsing it\rpw \u0026lt;- {\r\u0026quot;correcthorsebatterystaple\u0026quot;\r}\r# loads the PostgreSQL driver\rdrv \u0026lt;- dbDriver(\u0026quot;PostgreSQL\u0026quot;)\r# creates a connection to the postgres database\r# note that \u0026quot;con\u0026quot; will be used later in each connection to the database\rcon \u0026lt;- dbConnect(drv, dbname = \u0026quot;postgres\u0026quot;,\rhost = \u0026quot;localhost\u0026quot;, port = 5432,\ruser = \u0026quot;openpg\u0026quot;, password = pw)\rrm(pw) # removes the password\r# check for the cartable\rdbExistsTable(con, \u0026quot;cartable\u0026quot;)\r# TRUE\rIf we don’t get an error, that means we are connected to the database.\nWrite and Load Data with RPostgreSQL\rThe following code show how we can write and read data to the database:\n# creates df, a data.frame with the necessary columns\rdata(mtcars)\rdf \u0026lt;- data.frame(carname = rownames(mtcars), mtcars, row.names = NULL)\rdf$carname \u0026lt;- as.character(df$carname)\rrm(mtcars)\r# writes df to the PostgreSQL database \u0026quot;postgres\u0026quot;, table \u0026quot;cartable\u0026quot; dbWriteTable(con, \u0026quot;cartable\u0026quot;, value = df, append = TRUE, row.names = FALSE)\r# query the data from postgreSQL df_postgres \u0026lt;- dbGetQuery(con, \u0026quot;SELECT * from cartable\u0026quot;)\r# compares the two data.frames\ridentical(df, df_postgres)\r# TRUE\r# Basic Graph of the Data\rrequire(ggplot2)\rggplot(df_postgres, aes(x = as.factor(cyl), y = mpg, fill = as.factor(cyl))) + geom_boxplot() + theme_bw()\rLastly, if we are finished, we have to disconnect from the server:\n# close the connection\rdbDisconnect(con)\rdbUnloadDriver(drv)\rOutro\rIf you have any questions about the code, PostgreSQL or pgAdmin III or if you have remarks or have found a way to do it better/faster feel free to leave a comment or write me an email.\nUseful links:\rGet the PostgreSQL software here:\rhttp://www.postgresql.org/download/windows/\nPostgreSQL commandline commands: http://www.postgresql.org/docs/9.4/static/app-pg-ctl.html\nCreate a new User: https://doc.odoo.com/install/windows/postgres/\nFor a short introduction to postgreSQL queries have a look at this: http://www.postgresql.org/docs/8.4/static/tutorial-select.html\nAppendix\rIf you want to create a table in R instead of pgAdmin III you can do that of course. The following creates the same table as the we did earlier in pgAdmin III:\n# specifies the details of the table\rsql_command \u0026lt;- \u0026quot;CREATE TABLE cartable\r(\rcarname character varying NOT NULL,\rmpg numeric(3,1),\rcyl numeric(1,0),\rdisp numeric(4,1), hp numeric(3,0),\rdrat numeric(3,2),\rwt numeric(4,3),\rqsec numeric(4,2),\rvs numeric(1,0),\ram numeric(1,0),\rgear numeric(1,0),\rcarb numeric(1,0),\rCONSTRAINT cartable_pkey PRIMARY KEY (carname)\r)\rWITH (\rOIDS=FALSE\r);\rALTER TABLE cartable\rOWNER TO openpg;\rCOMMENT ON COLUMN cartable.disp IS \u0026#39;\r\u0026#39;;\u0026quot;\r# sends the command and creates the table\rdbGetQuery(con, sql_command)\rAppendix 2: Improvements\rAlmost a year after I wrote this blog post, I have made my own fair amount of experience with SQL and wanted to share some experience with you.\nFirst: You may want to write a short function that connects you to the server and makes sure the connection closes again. What I usually use is something like this:\n# 1) for quering from SQL\rSQLCommand \u0026lt;- function(query){\ron.exit(dbDisconnect(con))\rcon \u0026lt;- dbConnect(drv = dbDriver(\u0026quot;PostgreSQL\u0026quot;), dbname = \u0026quot;postgres\u0026quot;, host = \u0026quot;localhost\u0026quot;, port = 5432, user = \u0026quot;openpg\u0026quot;, password = \u0026quot;correcthorsebatterystaple\u0026quot;)\rtmp \u0026lt;- dbGetQuery(con, query)\r}\r# 2) for writing to SQL\rSQLWriteValues \u0026lt;- function(values, table){\ron.exit(dbDisconnect(con))\rcon \u0026lt;- dbConnect(drv = dbDriver(\u0026quot;PostgreSQL\u0026quot;), dbname = \u0026quot;postgres\u0026quot;, host = \u0026quot;localhost\u0026quot;, port = 5432, user = \u0026quot;openpg\u0026quot;, password = \u0026quot;correcthorsebatterystaple\u0026quot;)\rdbWriteTable(con, table, value = values, append = T, row.names = F)\rreturn(NULL)\r}\rI would then use it like this:\nSQLWriteValues(df, \u0026quot;cartable\u0026quot;)\rdf2 \u0026lt;- SQLCommand(\u0026quot;SELECT * FROM cartable\u0026quot;)\rLastly, if you want to exclude the details of your connection such as username, password, etc. from your source code, because you work with an open github repository or you want to be able to change your passwords easily, I do the following, which outsources the connection to a config.yml (make sure that you include this file in your .gitignore) file and loads the necessary information using this:\nconfig.yml (make sure that this file is in your working directory):\ndb:\rhost : \u0026quot;localhost\u0026quot;\rdbname : \u0026quot;postgres\u0026quot;\ruser : \u0026quot;openpg\u0026quot;\rport : 5432\rpassword : \u0026quot;correcthorsebatterystaple\u0026quot;\rin your r-file:\nlibrary(yaml)\rSQLCommand \u0026lt;- function(query){\ron.exit(dbDisconnect(con))\rcon \u0026lt;- do.call(dbConnect, c(drv = dbDriver(\u0026quot;PostgreSQL\u0026quot;), yaml.load_file(\u0026quot;config.yml\u0026quot;)$db))\rtmp \u0026lt;- dbGetQuery(con, query)\rreturn(tmp)\r}\rSQLWriteValues \u0026lt;- function(values, table){\ron.exit(dbDisconnect(con))\rcon \u0026lt;- do.call(dbConnect, c(drv = dbDriver(\u0026quot;PostgreSQL\u0026quot;), yaml.load_file(\u0026quot;config.yml\u0026quot;)$db))\rdbWriteTable(con, table, value = values, append = T, row.names = F)\rreturn(NULL)\r}\r# same as before\rSQLWriteValues(df, \u0026quot;cartable\u0026quot;)\rdf2 \u0026lt;- SQLCommand(\u0026quot;SELECT * FROM cartable\u0026quot;)\r","date":1431907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431907200,"objectID":"7172764e9205969064ebb87b011ecc8a","permalink":"https://davzim.github.io/blog/2015-05-18-getting-started-postgresql/","publishdate":"2015-05-18T00:00:00Z","relpermalink":"/blog/2015-05-18-getting-started-postgresql/","section":"blog","summary":"When dealing with large datasets that potentially exceed the memory of your machine it is nice to have another possibility such as your own server with an SQL/PostgreSQL database on it, where you can query the data in smaller digestible chunks.","tags":["R","database","postgres","RPostgreSQL"],"title":"Getting started with PostgreSQL in R","type":"blog"},{"authors":null,"categories":["R"],"content":"\rWarning\rAfter revisiting this blog 5 years later, I was not able to reproduce the code fully, see the addendum for a post-mortem.\nIntroduction\rRecently I found a good introduction to the Schelling-Segregation Model and to Agent Based Modelling (ABM) for Python (Binpress Article by Adil). The model follows an ABM approach to simulate how urban segregation can be explained. I will concentrate on the R-code, if you want to know more about the Schelling-Segregation Model (which brought its creator a Nobel Price) and Agent Based Modelling you should head over to the binpress page! As my Python knowledge is not sufficiently large enough, I try to rebuild the ABM in R with some guidelines from the Python script, but as I use data.table and its specific functions, the code naturally deviates quite a lot.\nSchelling-Segregation Model\rThe idea behind the Schelling Model is that we create an M x N grid that contains homes for our agents, which we simulate to belong to n different races, with a certain possibility that homes are empty. In each round I calculate a ratio of same-race neighbors over the total amount neighbors for each home (position on the grid). If the ratio falls below a certain threshold (in this case 50%), the agent becomes unhappy and will move to another (random) home. This process is iterated to find an equilibrium.\nBasic Principles of the Code\rThe ABM is based on three self-written functions:\ninitiateSchelling()\rtakes inputs of dimensions of the simulated area and the number of different races\rcreates a data.table called schelling that contains the id for each position in the grid, x and y the coordinates, the race as well as distance and unsatisfied, which we will use later\rplotSchelling()\rtakes a text input that is used as the graph’s title\rit uses the schelling data.table and plots each agent\riterate()\rtakes the number of iterations (= number of simulations) as well as the similarity threshold\rthe function has another subfunction is.unsatisfied() that checks for each row if the agent is unsatisfied\riterate first checks for each agent if she is unsatisfied, if so the agent will be moved\rR-Code of the Functions\rTo fasten up the speed of the code, I use the data.table package including some of its specialties. If you are unfamiliar with the syntax of data.table, I recommend you to have a look at this excellent intro by yhat or the CheatSheet by DataCamp. For visualization, I use ggplot2 and RColorBrewer. The packages are loaded with:\nlibrary(data.table)\rlibrary(ggplot2)\rThe code for the initiateSchelling()-function looks like this:\ninitiateSchelling \u0026lt;- function(dimensions = c(10, 10), n_races = 2){\r# create \u0026quot;races\u0026quot; based on colours\rraces \u0026lt;- colours()[1:n_races]\r# what percentage of the homes will be empty\rperc_empty \u0026lt;- 0.2\r# how many homes will be simulated\rn_homes = prod(dimensions)\r# calculates the number of agents\rcount_agents \u0026lt;- floor(n_homes * (1 - perc_empty))\r# the characteristics that a home can have\rraces \u0026lt;- c(\u0026quot;empty\u0026quot;, races)\r# the probabilities of each characteristics\rprobabilities \u0026lt;- c(perc_empty, rep((1 - perc_empty)/(length(races) - 1),\rtimes = length(races) - 1))\r# creates the global schelling data.table\rschelling \u0026lt;\u0026lt;- data.table(id = 1:prod(dimensions),\rx = rep(1:dimensions[1],\rdimensions[2]),\ry = rep(1:dimensions[2],\reach = dimensions[1]),\rrace = sample(x = races,\rsize = n_homes,\rprob = probabilities,\rreplace = TRUE),\r# used to find the satisfaction of each home\rdistance = rep(NA, prod(dimensions)),\runsatisfied = rep(NA, prod(dimensions)))\r}\rSecondly, the plotSchelling()-function looks like this:\nplotSchelling \u0026lt;- function(title = \u0026quot;Schelling-Segregation-Model\u0026quot;){\r# get races to get the right number of colors\rraces \u0026lt;- unique(schelling$race)\r# find the dimensions of the grid to get the best dot size\rdims \u0026lt;- c(max(schelling$x), max(schelling$y))\r# plot the graph\rp \u0026lt;- ggplot(data = schelling[race != \u0026quot;empty\u0026quot;],\raes(x = x, y = y, color = race)) +\r# workaround to get relatively large dots that\r# resize with the size of the grid\rgeom_point(size = 100/sqrt(prod(dims))) +\rscale_colour_brewer(\u0026quot;Dark2\u0026quot;) +\r# create a beatiful and mostly empty chart\rtheme_bw() +\rtheme(axis.line = element_blank(),\raxis.text.x = element_blank(),\raxis.text.y = element_blank(),\raxis.ticks = element_blank(),\raxis.title.x = element_blank(),\raxis.title.y = element_blank(),\rlegend.position = \u0026quot;none\u0026quot;,\rpanel.background = element_blank(),\rpanel.border = element_blank(),\rpanel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(),\rplot.background = element_blank(),\rplot.title = element_text(lineheight=3,\rface=\u0026quot;bold\u0026quot;,\rcolor=\u0026quot;black\u0026quot;,\rsize=14)) +\r# fixes the axis to avoid distortion in the picture\rcoord_fixed(ratio = 1) +\r# lastly adds the title\rggtitle(title)\rreturn(p)\r}\rAnd lastly, the iterate()-function, that iterates over the checks for satisfaction and moves of the agents if necessary, looks like this:\niterate \u0026lt;- function(n = 10, similiarity_threshold){\r# subfunction that checks for a given x and y value if the agent is\r# unsatisfied (returns TRUE or FALSE)\ris.unsatisfied \u0026lt;- function(x_value, y_value, similiarity_threshold = 0.5){\r# gets the race for the agent\rcur_race \u0026lt;- schelling[x == x_value \u0026amp; y == y_value, race]\r# checks if the home is empty to\rif (cur_race == \u0026quot;empty\u0026quot;){\rreturn(FALSE) # empty houses are not satisfied, therefore will not move!\r} else {\r# creates the square of the distance\r# I avoid to take the squareroot to speed up the code\rschelling[, distance := (x_value - x)^2 + (y_value - y)^2] # counts the number of agents that live less than two fields away\r# (includes all adjacent agents) and that are similar\rcount_similar \u0026lt;- nrow(schelling[distance \u0026lt;= 2 \u0026amp;\u0026amp;\rrace == cur_race \u0026amp;\u0026amp;\rdistance != 0])\r# same here except that it looks into different agents\rcount_different \u0026lt;- nrow(schelling[distance \u0026lt;= 2 \u0026amp;\u0026amp;\rrace != cur_race \u0026amp;\u0026amp;\rrace != \u0026quot;empty\u0026quot;])\r# calculates the ratio\rratio \u0026lt;- count_similar/(count_similar + count_different)\r# returns TRUE if the ratio is below the threshold\rreturn(ratio \u0026lt; similiarity_threshold)\r}\r}\r# creates a ProgressBar, although this is not necessary, it does look nice..\rpb \u0026lt;- txtProgressBar(min = 0, max = 1, style = 3)\r# for time-keeping-purposes\rt \u0026lt;- Sys.time()\r# iterates\rfor (iterate in 1:n){\r# fills the boolean vector \u0026quot;unsatisfied\u0026quot;\r# indicates if the household is unsatisfied\rschelling[, unsatisfied := is.unsatisfied(x_value = x,\ry_value = y,\rsimiliarity_threshold =\rsimiliarity_threshold),\rby = 1:nrow(schelling)]\r# move unsatisfied agents to an empty house\r# find the IDs that are empty where agents can migrate to\remptyIDs \u0026lt;- schelling[race == \u0026quot;empty\u0026quot;, id] # finds the id of empty houses\r# finds the origin of the agents moving,\r# aka. the old ID of each household moving\roldIDs \u0026lt;- schelling[(unsatisfied), id] # origin\r# generates new IDs for each household moving\r# note that households can move to the house of other moving agents\r# also, agents can (by a small chance) choose to \u0026quot;move\u0026quot; to their\r# existing home\rnewIDs \u0026lt;- sample(x = c(emptyIDs, oldIDs),\rsize = length(oldIDs),\rreplace = F) # target\r# a new data.table that shows\r# what race migrates from which origin_id to which target-id\rtransition \u0026lt;- data.table(origin = oldIDs,\roldRace = schelling[id %in% oldIDs, race],\rtarget = newIDs)\r# moves the agents to the new homes\rschelling[id %in% transition$origin]$race = \u0026quot;empty\u0026quot;\rschelling[id %in% transition$target]$race = transition$oldRace\r# orders the schelling, although this takes some time,\r# it is necessary for the other operations\rschelling \u0026lt;- schelling[order(id)]\r# updates the ProgressBar\rsetTxtProgressBar(pb, iterate/n)\r}\rclose(pb)\rtimedif \u0026lt;- Sys.time() - t\r# print out statistics for the calculation time\rprint(paste0(\u0026quot;Time for calculation in seconds: \u0026quot;, round(timedif, 3), \u0026quot; or: \u0026quot;,\rround(n / as.numeric(timedif), 3), \u0026quot; iterations per second\u0026quot;))\rreturn(schelling)\r}\rResults 1: 2 Races\rBy using the function I create a 50x50 grid with 2.500 agents and simulate 20 rounds (this process takes roughly a minute). A visualization is produced at 0, 10, and 20 iterations; after 20 rounds the 2-race simulation reaches an equilibrium as we can see by the few changes between the two states (10 and 20 iterations).\nset.seed(42^2)\r# initiate schelling\rinitiateSchelling(dimensions = c(50, 50), n_races = 2)\r# plot schelling\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 0 iterations\u0026quot;)\r# iterate 10 times\rschelling \u0026lt;- iterate(n = 10, similiarity_threshold = 0.5)\r# plot the result after 10 iterations\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 10 iterations\u0026quot;)\r# iterate another 10 times\rschelling \u0026lt;- iterate(n = 10, similiarity_threshold = 0.5)\r# plot again after 20 iterations total\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 20 iterations\u0026quot;)\rAlthough it seems that the model found an equilibrium after 10 iterations, there are still some minor changes between the two states, albeit only few.\nResults 2: 4 Races\rTo see the ABM with 4 different races I used the following code to generate the following images.\nset.seed(42^3)\r# initiate schelling\rinitiateSchelling(dimensions = c(50, 50), n_races = 4)\r# plot schelling\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 0 iterations\u0026quot;)\r# iterate 10 times\rschelling \u0026lt;- iterate(n = 10, similiarity_threshold = 0.5)\r# plot the result after 10 iterations\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 10 iterations\u0026quot;)\rA more notable change happens between the states after 10 and 20 iterations.\n# iterate another 10 times\rschelling \u0026lt;- iterate(n = 10, similiarity_threshold = 0.5)\r# plot again after 20 iterations total\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 20 iterations\u0026quot;)\r# iterate another 30 times\rschelling \u0026lt;- iterate(n = 30, similiarity_threshold = 0.5)\r# plot again after 50 iterations total\rplotSchelling(title = \u0026quot;Schelling-Segregation-Model after 50 iterations\u0026quot;)\rHere we see that the model took more iterations to find an almost steady-state after 50 iterations (there are still some agents that will move in the next round, can you spot them?).\nOutro\rThese few lines show nicely what ABMs are, how they can be used and how they simplify complex human behavior. Although data.table is enormously fast, the process still takes a lot of time to calculate. If you have any idea how to speed up the code, I would love to hear about it in the comments! If you have any other questions or remarks, I am of course happy to hear about them in the comments as well. Lastly, if you want to see other implementations of the Schelling-Segregation Model in R, you can visit R Snippets or R-bloggers.\nAddendum from 2020\rAs noted in the blogpost, this code is horribly slow and does not produce the plots I advertised originally.\rThis is, of course, not data.table’s or R’s fault, but past me’s.\rRevisiting the code leads to some cringe moments for me as lots of pieces can be made more clearly and more efficient.\rI guess I have learned a lot in the last 5 years.\nHere are a few things I would do differently as of 2020\nnot use global variables (i.e., not use \u0026lt;\u0026lt;-), instead each function should take the data as its first argument\respecially ABMs lend themselves to object-oriented programming as each state (of agents) depends on other states. Therefore R6 might be worth investigating.\ruse a different code style: initiate_schelling() (snake_case) instead of initiateSchelling() (camelCase)\rBut most importantly:\nAs this is a good exercise for more efficient programming.\rFor example, each round filters the data table over, creating unnecessary computation time.\rTherefore, I ported the code to Rcpp and created a package some time ago.\rIntroducing the SchellingR package.\nWhich has some nice features, e.g., emojis and/or animations as well as very fast execution thanks to Rcpp\nlibrary(SchellingR)\rset.seed(1234567)\r# run the schelling model on a 10x10 grid\rsh \u0026lt;- run_schelling(size = 10, percent_empty = 0.2, threshold = 0.5, number_of_groups = 2,\rmax_rounds = 100)\rplot_grid(sh, select_round = 8, title = TRUE, emoji = TRUE)\ror even better\nplot_grid(sh, select_round = 8, title = TRUE, emoji = TRUE, animate = TRUE)\r","date":1431388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431388800,"objectID":"ef6a3d71282572ba43977940665abfc9","permalink":"https://davzim.github.io/blog/2015-05-12-abm-with-rdatatable/","publishdate":"2015-05-12T00:00:00Z","relpermalink":"/blog/2015-05-12-abm-with-rdatatable/","section":"blog","summary":"Warning\rAfter revisiting this blog 5 years later, I was not able to reproduce the code fully, see the addendum for a post-mortem.\nIntroduction\rRecently I found a good introduction to the Schelling-Segregation Model and to Agent Based Modelling (ABM) for Python (Binpress Article by Adil).","tags":["data.table","agent-based modelling","ggplot2","R"],"title":"Agent Based Modelling with data.table OR how to model urban migration with R","type":"blog"},{"authors":null,"categories":["R"],"content":"\rFor a project I recently faced the issue of getting a database of all aviation incidents. As I really wanted to try Hadley’s new rvest-package, I thought I will give it a try and share the code with you.\nThe data of aviation incidents starting in 1919 from the Aviation Safety Network can be found here: Aviation Safety Network\nFirst, we needed to install and load the rvest-package, as well as dplyr, which I love for removing lots of messy code (if you are unfamiliar with the piping-operator %\u0026gt;% have a look at this description: Introduction to Piping in R\n# install.packages(\u0026quot;rvest\u0026quot;)\r# install.packages(\u0026quot;dplyr\u0026quot;)\rlibrary(rvest)\r## Loading required package: xml2\rlibrary(dplyr)\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\rLet’s try out some functions of rvest.\nSay we want to read all incidents that happened in the year 1920: Aviation Incidents 1920. We need to find the right html table to download and the link to it, to be more precise, the XPath. This can be done by using “inspect element” (right-click on the table, inspect element, right click on the element in the code and “copy XPath”). In our case the XPath is\r\u0026quot;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/table\u0026quot;.\rTo load the html data to R we can use:\nurl \u0026lt;- \u0026quot;http://aviation-safety.net/database/dblist.php?Year=1920\u0026quot;\r# load the html code to R\rincidents1920 \u0026lt;- url %\u0026gt;% read_html() # filter for the right xpath node\rincidents1920 \u0026lt;- incidents1920 %\u0026gt;% html_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/table\u0026#39;) # convert to a data.frame\rincidents1920 \u0026lt;- incidents1920 %\u0026gt;% html_table() %\u0026gt;% data.frame()\r# or in one go\rincidents1920 \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;% html_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/table\u0026#39;) %\u0026gt;% html_table() %\u0026gt;% data.frame()\rWhich gives us a small data.frame of 4 accidents.\nBut what happens if we have more than one page of data per year? We certainly don’t want to paste everything by hand. Take 1962 for example Accidents in 1962, which has 3 pages. Luckily we can get the number of pages by using rvest as well.\nWe follow the steps above to get the number of pages per year with the XPath “//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/div[2]”, with some cleaning we get the maximum pagenumber as:\nurl \u0026lt;- \u0026quot;http://aviation-safety.net/database/dblist.php?Year=1962\u0026quot;\rpages \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;%\rhtml_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/div[2]\u0026#39;) %\u0026gt;%\rhtml_text() %\u0026gt;% strsplit(\u0026quot; \u0026quot;) %\u0026gt;% unlist() %\u0026gt;%\ras.numeric() %\u0026gt;% max()\rpages\r## [1] 3\rNow we can write a small loop to get all incidents of 1962, as the link changes with the page number, ie from:\rhttp://aviation-safety.net/database/dblist.php?Year=1962\u0026amp;lang=\u0026amp;page=1\rto\rhttp://aviation-safety.net/database/dblist.php?Year=1962\u0026amp;lang=\u0026amp;page=2\nThe code for the loop looks like this:\n# initiate empty data.frame, in which we will store the data\rdat \u0026lt;- data.frame(\rdate = numeric(0), type = numeric(0), registration = numeric(0),\roperator = numeric(0), fatalities = numeric(0),\rlocation = numeric(0), category = numeric(0)\r)\r# loop through all page numbers\rfor (page in 1:pages){\r# create the new URL for the current page\rurl \u0026lt;- paste0(\r\u0026quot;http://aviation-safety.net/database/dblist.php?Year=1962\u0026amp;lang=\u0026amp;page=\u0026quot;, page\r)\r# get the html data and convert it to a data.frame\rincidents \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;%\rhtml_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/table\u0026#39;) %\u0026gt;%\rhtml_table() %\u0026gt;% data.frame()\r# combine the data\rdat \u0026lt;- rbind(dat, incidents)\r}\r# quick look at the dimens\rdim(dat)\r## [1] 236 9\rhead(dat)\r## date type registration operator fat.\r## 1 02-JAN-1962 Douglas C-47 (DC-3) EP-ABB Iran Air 0\r## 2 02-JAN-1962 Convair C-131E (CV-440) 55-4750 USAF ## 3 06-JAN-1962 Curtiss C-46A HR-TNB TAN 1\r## 4 08-JAN-1962 Lockheed RB-69A Neptune 54-4038 Republic of China AF 14\r## 5 10-JAN-1962 Douglas C-47 (DC-3) Air National Guard 5\r## 6 10-JAN-1962 Douglas C-47A (DC-3) 42-92078 USAF ## location Var.7 pic cat\r## 1 Kabul-Khwaja... NA NA A1\r## 2 NA NA U1\r## 3 Belize City-... NA NA A1\r## 4 Korea Bay NA NA A1\r## 5 South Park T... NA NA A1\r## 6 unknown NA NA A1\rwhich gives us a data.frame consisting of 211 incidents of the year 1962.\nLastly, we can write a loop to gather the data over multiple years:\n# set-up of initial values\rstartyear \u0026lt;- 1960\rendyear \u0026lt;- 1965\rurl_init \u0026lt;- \u0026quot;http://aviation-safety.net/database/dblist.php?Year=\u0026quot;\r# initiate empty dataframe, in which we will store the data\rdat \u0026lt;- data.frame(\rdate = numeric(0), type = numeric(0), registration = numeric(0),\roperator = numeric(0), fatalities = numeric(0),\rlocation = numeric(0), category = numeric(0)\r)\rfor (year in startyear:endyear){\r# get url for this year\rurl_year \u0026lt;- paste0(url_init, year)\r# get pages\rpages \u0026lt;- url_year %\u0026gt;% read_html() %\u0026gt;%\rhtml_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/div[2]\u0026#39;) %\u0026gt;%\rhtml_text() %\u0026gt;% strsplit(\u0026quot; \u0026quot;) %\u0026gt;% unlist() %\u0026gt;%\ras.numeric() %\u0026gt;% max()\r# loop through the pages\rfor (page in 1:pages){\rurl \u0026lt;- paste0(url_year,\u0026quot;\u0026amp;lang=\u0026amp;page=\u0026quot;, page)\r# get the html data and convert it to a data.frame\rincidents \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;%\rhtml_nodes(xpath = \u0026#39;//*[@id=\u0026quot;contentcolumnfull\u0026quot;]/div/table\u0026#39;) %\u0026gt;%\rhtml_table() %\u0026gt;% data.frame()\r# combine the data\rdat \u0026lt;- rbind(dat, incidents)\r}\r}\rdim(dat)\r## [1] 1363 9\rhead(dat)\r## date type registration operator\r## 1 03-JAN-1960 Douglas C-47A (DC-3) VT-CGG Indian Airlines\r## 2 03-JAN-1960 Lockheed L-749A Constellation N110A Eastern Air Lines\r## 3 04-JAN-1960 Curtiss C-46A PP-SLJ T.A. Salvador\r## 4 04-JAN-1960 U-1A Otter (DHC-3) 55-2974 US Army\r## 5 05-JAN-1960 Vickers 701 Viscount G-AMNY BEA\r## 6 06-JAN-1960 Douglas DC-6B N8225H National Airlines\r## fat. location Var.7 pic cat\r## 1 9 near Taksing NA NA A1\r## 2 0 Philadelphia... NA NA A1\r## 3 0 Dianopolis A... NA NA A1\r## 4 10 Gulf of Sirte NA NA A1\r## 5 0 Malta-Luqa A... NA NA A1\r## 6 34 near Bolivia, NC NA NA C1\rIn the years 1960-1965 there were 1.363 recorded aviation incidents, which we can now use in R.\n","date":1430352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430352000,"objectID":"e94ccb52bb3167c0c19b02e33ea7124d","permalink":"https://davzim.github.io/blog/2015-04-30-rvest-aviation/","publishdate":"2015-04-30T00:00:00Z","relpermalink":"/blog/2015-04-30-rvest-aviation/","section":"blog","summary":"For a project I recently faced the issue of getting a database of all aviation incidents. As I really wanted to try Hadley’s new rvest-package, I thought I will give it a try and share the code with you.","tags":["dplyr","rvest","web-scraping","aviation","R"],"title":"Using rvest and dplyr to scrape data and look at aviation incidents","type":"blog"}]