<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>finance | Data Shenanigans</title>
    <link>/tag/finance/</link>
      <atom:link href="/tag/finance/index.xml" rel="self" type="application/rss+xml" />
    <description>finance</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2020</copyright><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo.svg</url>
      <title>finance</title>
      <link>/tag/finance/</link>
    </image>
    
    <item>
      <title>RITCH</title>
      <link>/project/ritch/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/project/ritch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introducing RITCH: Parsing ITCH Files in R (Finance &amp; Market Microstructure)</title>
      <link>/blog/2017-11-30-introducing-ritch/</link>
      <pubDate>Thu, 30 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/blog/2017-11-30-introducing-ritch/</guid>
      <description>


&lt;p&gt;Recently I was faced with a file compressed in NASDAQ’s ITCH-protocol, as I wasn’t able to find an R-package that parses and loads the file to R for me, I spent (probably) way to much time to write one, so here it is.
But you might wonder, what exactly is ITCH and why should I care? Well, ITCH is the outbound protocol NASDAQ uses to communicate market data to its clients, that is, all information including market status, orders, trades, circuit breakers, etc.
with nanosecond timestamps for each day and each exchange.
Kinda a must-have if you are looking into market microstructure, a good-to-have-looked-into-it if you are interested in general finance and/or if you are interested in data analytics and large structured datasets.
If you are wondering where you might get some of these fancy datasets in the first place, I have good news for you.
NASDAQ provides some sample datasets (6 days for 3 exchanges (NASDAQ, PSX, and BX), together about 25GBs gzipped) on its FTP server: &lt;a href=&#34;ftp://emi.nasdaq.com/ITCH/&#34;&gt;ftp://emi.nasdaq.com/ITCH/&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;the-ritch-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The RITCH Package&lt;/h2&gt;
&lt;p&gt;Now that I (hopefully) have your attention, let me represent to you &lt;code&gt;RITCH&lt;/code&gt; an R package that parses ITCH-files (version 5.0).
Currently the package only lives on GitHub (&lt;a href=&#34;https://github.com/DavZim/RITCH&#34;&gt;https://github.com/DavZim/RITCH&lt;/a&gt;), but it should find its way into CRAN eventually.
Until then, you have to use &lt;code&gt;devtools&lt;/code&gt; or &lt;code&gt;remotes&lt;/code&gt; to install it&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(&amp;quot;DavZim/RITCH&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And you should be good to go (if not, please let me know!).
RITCH so far has a very limited scope: extracting the messages from the ITCH-file plus some functions to count messages.
The package leverages &lt;code&gt;C++&lt;/code&gt; and the excellent &lt;code&gt;Rcpp&lt;/code&gt; library to optimise parsing.
RITCH itself does not contain any data as the datasets are too large for any repos and I have no copyright on the datasets in any way.
For the following code I will use the &lt;code&gt;20170130.BX_ITCH_50&lt;/code&gt;-file from NASDAQ’s FTP-server, as its not too large at 714MB gzipped (1.6GB gunzipped), but still has almost 55 million messages.&lt;/p&gt;
&lt;p&gt;All functions can take the gzipped or unzipped files, but if you use the file more than once and hard-drive space is not of utmost concern, I suggest you gunzip the file by hand (i.e., use &lt;code&gt;R.utils::gunzip(file, new_file, remove = FALSE)&lt;/code&gt; in R or &lt;code&gt;gunzip -k YYYYMMDD.XXX_ITCH_50.gz&lt;/code&gt; in the terminal) and call the functions on the “plain”-file.
I will address some concerns to size and speed later on.&lt;/p&gt;
&lt;p&gt;To download and prepare the data in R, we can use the following code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- &amp;quot;20170130.BX_ITCH_50.gz&amp;quot;
# might take some time as it downloads 714MB
if (!file.exists(file)) 
  download.file(&amp;quot;ftp://emi.nasdaq.com/ITCH/20170130.BX_ITCH_50.gz&amp;quot;, 
                file, mode = &amp;quot;wb&amp;quot;)

# gunzip the file, but keep the original file
R.utils::gunzip(&amp;quot;20170130.BX_ITCH_50.gz&amp;quot;, &amp;quot;20170130.BX_ITCH_50&amp;quot;, remove = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we want to get a general overview of the file, which we can do with &lt;code&gt;count_messages()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RITCH)
file &amp;lt;- &amp;quot;20170130.BX_ITCH_50&amp;quot;

msg_count &amp;lt;- count_messages(file, add_meta_data = TRUE) 
#&amp;gt; [Counting]   54473386 messages found
#&amp;gt; [Converting] to data.table

msg_count
#&amp;gt;     msg_type    count                                  msg_name                                    msg_group  doc_nr
#&amp;gt;  1:        S        6                      System Event Message                         System Event Message     4.1
#&amp;gt;  2:        R     8371                           Stock Directory                       Stock Related Messages   4.2.1
#&amp;gt;  3:        H     8401                      Stock Trading Action                       Stock Related Messages   4.2.2
#&amp;gt;  4:        Y     8502                       Reg SHO Restriction                       Stock Related Messages   4.2.3
#&amp;gt;  5:        L     6011               Market Participant Position                       Stock Related Messages   4.2.4
#&amp;gt;  6:        V        2                MWCB Decline Level Message                       Stock Related Messages 4.2.5.1
#&amp;gt;  7:        W        0                       MWCB Status Message                       Stock Related Messages 4.2.5.2
#&amp;gt;  8:        K        0                 IPO Quoting Period Update                       Stock Related Messages   4.2.6
#&amp;gt;  9:        J        0                       LULD Auction Collar                       Stock Related Messages   4.2.7
#&amp;gt; 10:        A 21142017                         Add Order Message                            Add Order Message   4.3.1
#&amp;gt; 11:        F    20648      Add Order - MPID Attribution Message                            Add Order Message   4.3.2
#&amp;gt; 12:        E  1203625                    Order Executed Message                        Modify Order Messages   4.4.1
#&amp;gt; 13:        C     8467 Order Executed Message With Price Message                        Modify Order Messages   4.4.2
#&amp;gt; 14:        X  1498904                      Order Cancel Message                        Modify Order Messages   4.4.3
#&amp;gt; 15:        D 20282644                      Order Delete Message                        Modify Order Messages   4.4.4
#&amp;gt; 16:        U  3020278                     Order Replace Message                        Modify Order Messages   4.4.5
#&amp;gt; 17:        P   330023                 Trade Message (Non-Cross)                               Trade Messages   4.5.1
#&amp;gt; 18:        Q        0                       Cross Trade Message                               Trade Messages   4.5.2
#&amp;gt; 19:        B        0                      Broken Trade Message                               Trade Messages   4.5.3
#&amp;gt; 20:        I        0                              NOII Message Net Order Imbalance Indicator (NOII) Message     4.6
#&amp;gt; 21:        N  6935487                   Retail Interest Message    Retail Price Improvement Indicator (RPII)     4.7
#&amp;gt;     msg_type    count                                  msg_name                                    msg_group  doc_nr&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there are a lot of different message types.&lt;/p&gt;
&lt;p&gt;Currently this package parses only messages from the group “Add Order Messages” (type ‘A’ and ‘F’), “Modify Order Messages” (type ‘E’, ‘C’, ‘X’, ‘D’, and ‘U’), and “Trade Messages” (type ‘P’, ‘Q’, and ‘B’).
You can extract the different message-types by using the functions &lt;code&gt;get_orders&lt;/code&gt;, &lt;code&gt;get_modifications&lt;/code&gt;, and &lt;code&gt;get_trades&lt;/code&gt;, respectively.
The doc-number refers to the section in the &lt;a href=&#34;https://www.nasdaqtrader.com/content/technicalsupport/specifications/dataproducts/NQTVITCHspecification.pdf&#34;&gt;official documentation&lt;/a&gt; (which also contains more detailed description what each type contains).
If you are annoyed by the feedback the function gives you (&lt;code&gt;[Counting] ... [Converting]...&lt;/code&gt;), you can always turn the feedback off with the &lt;code&gt;quiet = TRUE&lt;/code&gt; option (this applies to all functions).
Lets try to parse the first 10 orders&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;orders &amp;lt;- get_orders(file, 1, 10) 
#&amp;gt; 10 messages found
#&amp;gt; [Loading]    .
#&amp;gt; [Converting] to data.table
#&amp;gt; [Formatting]

orders
#&amp;gt;     msg_type locate_code tracking_number    timestamp order_ref   buy shares stock   price mpid       date            datetime
#&amp;gt;  1:        A        7584               0 2.520001e+13     36132  TRUE 500000  UAMY  0.0001   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  2:        A        3223               0 2.520001e+13     36133  TRUE 500000  GLOW  0.0001   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  3:        A        2937               0 2.520001e+13     36136 FALSE    200   FRP 18.6500   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  4:        A        5907               0 2.520001e+13     36137  TRUE   1500   PIP  3.1500   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  5:        A        5907               0 2.520001e+13     36138 FALSE   2000   PIP  3.2500   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  6:        A        5907               0 2.520001e+13     36139  TRUE   3000   PIP  3.1000   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  7:        A        5398               0 2.520001e+13     36140  TRUE    200   NSR 33.0000   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  8:        A        5907               0 2.520001e+13     36141 FALSE    500   PIP  3.2500   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt;  9:        A        2061               0 2.520001e+13     36142 FALSE   1300  DSCI  7.0000   NA 2017-01-30 2017-01-30 07:00:00
#&amp;gt; 10:        A        1582               0 2.520001e+13     36143  TRUE    500  CPPL 17.1500   NA 2017-01-30 2017-01-30 07:00:00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same works for trades using the &lt;code&gt;get_trades()&lt;/code&gt; function and for order modifications using the &lt;code&gt;get_modifications()&lt;/code&gt; function.
To speed up the &lt;code&gt;get_*&lt;/code&gt; functions, we can use the message-count information from earlier.
For example the following code yields the same results as above, but saves time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;orders &amp;lt;- get_orders(file, 1, count_orders(msg_count)) 
trades &amp;lt;- get_trades(file, 1, count_trades(msg_count)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to get more information about each field, you can have a look at the &lt;a href=&#34;https://www.nasdaqtrader.com/content/technicalsupport/specifications/dataproducts/NQTVITCHspecification.pdf&#34;&gt;official ITCH-protocol specification manual&lt;/a&gt; or you can get a small data.table about each message type by calling &lt;code&gt;get_meta_data()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;having-a-look-at-some-the-most-traded-etfs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Having a Look at some the most traded ETFs&lt;/h2&gt;
&lt;p&gt;To have at least one piece of eye-candy in this post, lets have a quick go at the orders and trades of SPY (an S&amp;amp;P 500 ETF and one of the most traded assets, in case you didn’t know), IWO (Russel 2000 Growth ETF), IWM (Russel 2000 Index ETF), and VXX (S&amp;amp;P 500 VIX ETF) on the BX-exchange.
In case you are wondering, I got these four tickers with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

get_orders(file, 1, count_orders(msg_count), quiet = T) %&amp;gt;% 
  .$stock %&amp;gt;% 
  table %&amp;gt;% 
  sort(decreasing = T) %&amp;gt;% 
  head(4)
#&amp;gt; .
#&amp;gt;    SPY    IWO    IWM    VXX 
#&amp;gt; 135119 135016 118123 117395&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we load the data (orders and trades) from the file, then we do some data munging, and finally plot the data using ggplot2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

# 0. load the data
orders &amp;lt;- get_orders(file, 1, count_orders(msg_count))
#&amp;gt; 21162665 messages found
#&amp;gt; [Loading]    ................
#&amp;gt; [Converting] to data.table
#&amp;gt; [Formatting]

trades &amp;lt;- get_trades(file, 1, count_trades(msg_count))
#&amp;gt; 330023 messages found
#&amp;gt; [Loading]    ................
#&amp;gt; [Converting] to data.table
#&amp;gt; [Formatting]

# 1. data munging
tickers &amp;lt;- c(&amp;quot;SPY&amp;quot;, &amp;quot;IWO&amp;quot;, &amp;quot;IWM&amp;quot;, &amp;quot;VXX&amp;quot;)
dt_orders &amp;lt;- orders[stock %in% tickers]
dt_trades &amp;lt;- trades[stock %in% tickers]

# for each ticker, use only orders that are within 1% of the range of traded prices
ranges &amp;lt;- dt_trades[, .(min_price = min(price), max_price = max(price)), by = stock]

# filter the orders
dt_orders &amp;lt;- dt_orders[ranges, on = &amp;quot;stock&amp;quot;][price &amp;gt;= 0.99 * min_price &amp;amp; price &amp;lt;= 1.01 * max_price]

# replace the buy-factor with something more useful
dt_orders[, buy := ifelse(buy, &amp;quot;Bid&amp;quot;, &amp;quot;Ask&amp;quot;)]
dt_orders[, stock := factor(stock, levels = tickers)]

# 2. data visualization
ggplot() +
  # add the orders to the plot
  geom_point(data = dt_orders,
             aes(x = datetime, y = price, color = buy), size = 0.5) +
  # add the trades as a black line to the plot
  geom_step(data = dt_trades,
            aes(x = datetime, y = price)) +
  # add a facet for each ETF
  facet_wrap(~stock, scales = &amp;quot;free_y&amp;quot;) +
  # some Aesthetics
  theme_light() +
  labs(title = &amp;quot;Orders and Trades of the largest ETFs&amp;quot;,
       subtitle = &amp;quot;Date: 2017-01-30 | Exchange: BX&amp;quot;,
       caption = &amp;quot;Source: NASDAQ&amp;quot;,
       x = &amp;quot;Time&amp;quot;, y = &amp;quot;Price&amp;quot;,
       color = &amp;quot;Side&amp;quot;) +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;etf_plot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now its up to you to do something interesting with the data, I hope RITCH can help you with it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;speed-ram-concerns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Speed &amp;amp; RAM concerns&lt;/h2&gt;
&lt;p&gt;If your machine struggles with some files, you can always load only parts of a file as shown above.
And of course, make sure that you have only necessary datasets in your R-environment and that no unused programs are open (Chrome with some open tabs in the background happily eats your RAM).
If your machine is able to handle larger datasets, you can increase the buffersize of each function call to 1GB or more using the &lt;code&gt;buffer_size = 1e9&lt;/code&gt; argument, increasing the speed with which a file is parsed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addendum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Addendum&lt;/h2&gt;
&lt;p&gt;If you find this package useful or have any other kind of feedback, I’d be happy if you let me know.
Otherwise, if you need more functionality for additional message types, please feel free to create an issue or a pull request on &lt;a href=&#34;https://github.com/DavZim/RITCH&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Importance of Out-of-Sample Tests and Lags in Forecasts and Trading Algorithms</title>
      <link>/blog/2016-10-24-oos-testing/</link>
      <pubDate>Mon, 24 Oct 2016 00:00:00 +0000</pubDate>
      <guid>/blog/2016-10-24-oos-testing/</guid>
      <description>


&lt;p&gt;I recently had the opportunity to listen to some great minds in the area of high-frequency data and trading.
While I won’t go into the details about what has been said, I wanted to illustrate the importance of proper out-of-sample testing and proper variable lags in potential trade algorithms or arbitrage models that has been brought up.
This topic can also be generalized to a certain degree to all forecasts.
The following example considers a case where some arbitrary trading algorithm is being tested: first without any proper tests, then with proper variable lags, and lastly using out-of-sample methodology.
I already downloaded the data (returns for all DJIA-components from 2010 to mid-sept 2016) and uploaded it to my &lt;a href=&#34;https://github.com/DavZim/Out-of-Sample-Testing&#34;&gt;github-page&lt;/a&gt;.
We can load the data like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
url &amp;lt;- &amp;quot;https://raw.githubusercontent.com/DavZim/Out-of-Sample-Testing/master/data/djia_data.csv&amp;quot;
df &amp;lt;- fread(url)
df[, date := as.Date(date)]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. The “Algorithm”&lt;/h2&gt;
&lt;p&gt;Say we have a simple algorithm that is supposed to predict the return of some stock (in this case AAPL) with some exogenous inputs (such as returns of other stocks in the Dow-Jones Industrial Average Index) using a linear model (not very realistic, but it serves the purpose for now).
We would take a long position if our model predicts positive returns and a short position otherwise.
If we control neither for out-of-sample, nor for lags, we receive a portfolio development which would look like this, pretty exciting isn’t it?!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scales)
# copy the data
df_none &amp;lt;- copy(df)

# train the model
mdl1 &amp;lt;- lm(AAPL ~ ., data = df_none[, !&amp;quot;date&amp;quot;, with = FALSE])

# predict the values (apply the model)
df_none[, AAPL_fcast := predict(mdl1, newdata = df_none)]

# calcaulate the earnings for the algorithm
df_none[, earnings := cumprod(1 + ifelse(sign(AAPL_fcast) &amp;gt; 0, AAPL, -AAPL))]

# plot the data
ggplot(df_none, aes(x = date, y = earnings)) + 
  geom_line(color = &amp;quot;#762a83&amp;quot;) + 
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Earnings Multiple&amp;quot;, title = &amp;quot;No Proper Tests&amp;quot;) +
  theme_light() +
  scale_y_continuous(labels = comma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-10-24-oos-testing_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Something’s fishy!
I wouldn’t expect a trading algorithm based on a simple linear regression to turn USD 1 initial investment into USD 25,876.65 after 6 years.
Why does that performance seem so wrong? That is because an algorithm at the time of prediction wouldn’t have access to the data that we trained it on.
To simulate a more realistic situation we can use lags and out-of-sample testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Lags&lt;/h2&gt;
&lt;p&gt;The key idea behind using lags is, that your trading algorithm only has access to the information it would have under realistic conditions, that is mostly, to predict the price of the next time unit &lt;span class=&#34;math inline&#34;&gt;\(t + 1\)&lt;/span&gt;, the latest information the algorithm can possibly have is from the current time unit &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.
For example, if we want to predict the price for tomorrow, we can only acces information from today.
To backtest our strategy, we will have to make sure that the algorithm sees only the lagged exogenous variables (the returns of all the other DJIA-components).
To save us some time, we can also take the lead of the endogenous variable as this is equivalent to lagging all other variables (except for the date-variable in this case, but that matters only for the visualization).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# copy and clean the data and calculate the lead
df_lag &amp;lt;- copy(df)
df_lag[, AAPL := shift(AAPL, type = &amp;quot;lead&amp;quot;)]
df_lag &amp;lt;- na.omit(df_lag)

# train the model
mdl_lag &amp;lt;- lm(AAPL ~ ., data = df_lag[, !&amp;quot;date&amp;quot;, with = FALSE])

# predict the new values
df_lag[, AAPL_lead_fcst := predict(mdl_lag, newdata = df_lag)]

# compute the earnings from the algorithm
df_lag[, earnings_lead := cumprod(1 + ifelse(sign(AAPL_lead_fcst) &amp;gt; 0, 
                                             AAPL, -AAPL))]

# convert into a data format that ggplot likes
df_plot &amp;lt;- df_lag[, .(date, AAPL_earnings = cumprod(1 + AAPL),
                      earnings_lead)]
df_plot &amp;lt;- melt(df_plot, id.vars = &amp;quot;date&amp;quot;)

# plot the data
ggplot(df_plot, aes(x = date, y = value, color = variable)) + 
  geom_line() +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Earnings Multiple&amp;quot;, title = &amp;quot;Lagged Variable&amp;quot;) +
  scale_color_manual(name = &amp;quot;Investment&amp;quot;, labels = c(&amp;quot;AAPL&amp;quot;, &amp;quot;Algorithm&amp;quot;),
                     values = c(&amp;quot;#1b7837&amp;quot;, &amp;quot;#762a83&amp;quot;)) + 
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-10-24-oos-testing_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That looks a bit more reasonable, though still unrealistic.
The algorithm outperforms the Apple stock by some magnitude of 6; an investment of USD 1 “only” gets turned into 25 (an annual growth rate of rougly 60%).
This unreasonable high return is due to the fact that we still haven’t conducted a proper out-of-sample test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;out-of-sample-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Out-of-Sample Testing&lt;/h2&gt;
&lt;p&gt;So far we have trained the model (or specified if you wish) on the same dataset that we validated the quality of the algorithm on.
On a meta-level you can think that the information which we use to test the algorithm is already used in the model, thus the model is to a certain degree a self-fulfilling prophecy (to a certain degree, this is also related to the concept of overfitting).
To avoid this, we can split the dataset into two samples for training and validation.
We use the training dataset to train the model and the validation dataset to test the algorithm “out-of-sample”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# copy and clean the data
df_oos &amp;lt;- copy(df)
df_oos[, AAPL := shift(AAPL, type = &amp;quot;lead&amp;quot;)]
df_oos &amp;lt;- na.omit(df_oos)

# split the data into training and validation sample
splitDate &amp;lt;- as.Date(&amp;quot;2015-12-31&amp;quot;)
df_training &amp;lt;- df_oos[date &amp;lt; splitDate]
df_validation &amp;lt;- df_oos[date &amp;gt;= splitDate]

# Train the model on the training dataset
mdl3 &amp;lt;- lm(AAPL ~ ., data = df_training[, !&amp;quot;date&amp;quot;, with = FALSE])

# In Sample (for comparison and plotting only) - NOT the out-of-sample test
df_training[, AAPL_fcast := predict(mdl3, newdata = df_training)]
df_training[, earnings_is := cumprod(1 + ifelse(sign(AAPL_fcast) &amp;gt; 0, 
                                                AAPL, -AAPL))]

# melt into a data format that ggplot likes
plot_df_is &amp;lt;- df_training[, .(date, AAPL = cumprod(1 + AAPL),
                              algorithm = earnings_is)]
plot_df_is &amp;lt;- melt(plot_df_is, id.vars = &amp;quot;date&amp;quot;)
plot_df_is[, type := &amp;quot;insample&amp;quot;]

# Out-of Sample Test
df_validation[, AAPL_oos_fcast := predict(mdl3, newdata = df_validation)]
df_validation[, &amp;#39;:=&amp;#39; (
  earnings_oos_lead = cumprod(1 + ifelse(sign(AAPL_oos_fcast) &amp;gt; 0, 
                                         AAPL, -AAPL))
)]

# melt into a data format that ggplot likes
plot_df_oos &amp;lt;- df_validation[, .(date,
                                 AAPL = cumprod(1 + AAPL), 
                                 algorithm = earnings_oos_lead)]
plot_df_oos &amp;lt;- melt(plot_df_oos, id.vars = &amp;quot;date&amp;quot;)
plot_df_oos[, type := &amp;quot;oos&amp;quot;]

plot_df &amp;lt;- rbindlist(list(plot_df_is, plot_df_oos))

# plot the data
facet_names &amp;lt;- c(&amp;quot;insample&amp;quot; = &amp;quot;In-Sample&amp;quot;, &amp;quot;oos&amp;quot; = &amp;quot;Out-of-Sample &amp;#39;16&amp;quot;)

ggplot(plot_df, aes(x = date, y = value, color = variable)) + 
  geom_line() +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Earnings Multiple&amp;quot;, 
       title = &amp;quot;Lagged Variable + Out-of-Sample Test&amp;quot;) +
  scale_color_manual(name = &amp;quot;Investment&amp;quot;, labels = c(&amp;quot;AAPL&amp;quot;, &amp;quot;Algorithm&amp;quot;),
                     values = c(&amp;quot;#1b7837&amp;quot;, &amp;quot;#762a83&amp;quot;)) + 
  theme_light() + 
  facet_wrap(~type, scales = &amp;quot;free&amp;quot;, labeller = as_labeller(facet_names))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-10-24-oos-testing_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the algorithm returns less than the stock it matches against (AAPL).
If we would invest in this simple algorithm, I would expect our investment to go down a lot more.
All of this is of course without considering trading, execution, and other transaction costs, which would even further decrease the returns of our trading algorithm.
To sum it up: If you are looking into algorithms and/or forecasts, always make sure that you apply proper out-of-sample tests and think about what information could have been used at the time of decision to avoid overfitting.
As always, if you find this interesting, find an error, or if you have a question you are more than welcome to leave a reply or contact me directly.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to Finance using R: Efficient Frontier and CAPM - Part 1</title>
      <link>/blog/2016-05-24-gentle-capm-i/</link>
      <pubDate>Tue, 24 May 2016 00:00:00 +0000</pubDate>
      <guid>/blog/2016-05-24-gentle-capm-i/</guid>
      <description>


&lt;p&gt;The following entry explains a basic principle of finance, the so-called efficient frontier and thus serves as a gentle introduction into one area of finance: “portfolio theory” using R.
A second part will then concentrate on the Capital-Asset-Pricing-Method (CAPM) and its assumptions, implications and drawbacks.&lt;/p&gt;
&lt;p&gt;Note: All code that is needed for the simulations, data manipulation, and plots that is not explicitly shown in this blogpost can be found on my &lt;a href=&#34;https://github.com/DavZim/Efficient_Frontier&#34;&gt;Github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the basic concepts of finance is the risk-return tradeoff.
As an illustrative example: If I were to offer you two investments with the same risk but with different expected returns, you would take the investment with the higher return (a closer look on what is meant by the terms risk and expected return follows later).
If the two investments had the same expected return but different levels of risk, most people would choose the investment with less risk.
This behaviour is called risk-aversion.&lt;/p&gt;
&lt;p&gt;But what happens if you have to make the decision between a strategy of how to distribute an investment worth of $100.000 between the 30 companies listed in the the German DAX index, the 100 companies from Britain’s FTSE, the 500 companies from the S&amp;amp;P 500, or if you have the (most realistic) option to invest in all listed stocks in the world?
How would you distribute the cash to create a promising portfolio?&lt;/p&gt;
&lt;p&gt;This is the question Markowitz tried to answer in 1952 in his paper “Portfolio Selection”.
Now, many years later, many academics and practitioners have added their answers to the discussion, advancing the discussion, but Markowitz’s solution is still considered to be a fundamental piece of finance.
His portfolio selection advanced into the now called “Capital Asset Pricing Model” or short CAPM.
The &lt;a href=&#34;http://www.rinfinance.com/agenda/&#34;&gt;R/Finance conference 2016&lt;/a&gt; alone lists at least 4 entries that are directly linked to portfolio theory and the CAPM.
So what exactly is the CAPM doing, what are its implications, assumptions, what are its drawbacks, and for us most importantly, how can we leverage R to calculate it?&lt;/p&gt;
&lt;p&gt;Let’s create a hands-on example to illustrate the theory:
We begin by looking at three different, well-known stocks of the Dow-Jones Industrial Average: IBM (&lt;code&gt;IBM&lt;/code&gt;), Google/Alphabet (&lt;code&gt;GOOG&lt;/code&gt;), and JP Morgan (&lt;code&gt;JPM&lt;/code&gt;).
I have gathered monthly data since 2000 from Yahoo using the &lt;code&gt;quantmod&lt;/code&gt;-library and saved it to my &lt;a href=&#34;https://github.com/DavZim/Efficient_Frontier&#34;&gt;Github-page&lt;/a&gt; (including the script to gather the data).
We can load the data and plot an indexed price with the following code (notice, I use &lt;code&gt;data.table&lt;/code&gt; for data storage and manipulation, but of course you can use &lt;code&gt;dplyr&lt;/code&gt;, or base-r as well)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(scales)
library(ggplot2)

link &amp;lt;- &amp;quot;https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/fin_data.csv&amp;quot;
dt &amp;lt;- fread(link)
dt[, date := as.Date(date)]

# create indexed values
dt[, idx_price := price/price[1], by = ticker]

# plot the indexed values
ggplot(dt, aes(x = date, y = idx_price, color = ticker)) +
  geom_line() +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Price Developments&amp;quot;) +
  xlab(&amp;quot;Date&amp;quot;) + ylab(&amp;quot;Price\n(Indexed 2000 = 1)&amp;quot;) +
  scale_color_discrete(name = &amp;quot;Company&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Altough we can see that Google outperformed the other two, we can also see that it seems it was risikier to invest in Google, as shown by the large spikes and drawdowns.
This is the principle of risk-return tradeoff that is fundamental to the CAPM.
Usually, risk is defined as volatility, which is defined as the standard deviation of the returns
.
On the other hand, return is either calculated using arithmetic or logarithmic returns (for simplicity I use arithmetic returns in this post, more on the mathematics of returns can be found in the appendix of this post).
To visualise the tradeoff, we can compute the mean and standard deviation for returns in a table or plot the two measures against each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate the arithmetic returns
dt[, ret := price / shift(price, 1) - 1, by = ticker]

# summary table
# take only non-na values
tab &amp;lt;- dt[!is.na(ret), .(ticker, ret)]

# calculate the expected returns (historical mean of returns) and
# volatility (standard deviation of returns)
tab &amp;lt;- tab[, .(er = round(mean(ret), 4), 
               sd = round(sd(ret), 4)), 
           by = &amp;quot;ticker&amp;quot;]
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ticker     er     sd
## 1:    IBM 0.0040 0.0554
## 2:   GOOG 0.0217 0.0801
## 3:    JPM 0.0064 0.0741&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table &lt;code&gt;tab&lt;/code&gt; shows that Google had the highest expected return and also the highest volatility, whereas IBM had the lowest figures, both in terms of expected returns and volatility.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Ticker&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Expected Return&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Volatility&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;IBM&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.41%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.55%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;GOOG&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.21%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.03%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;JPM&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.64%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.43%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;One visualisation you will have with no doubt already come across if you know a bit about the CAPM is a plot regarding this risk-return tradeoff, with the risk (volatility) on the x-axis, the expected returns on the y-axis and dots for each asset/company.
In our example it looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tab, aes(x = sd, y = er, color = ticker)) +
  geom_point(size = 5) +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Risk-Return Tradeoff&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, limits = c(0, 0.03)) +
  scale_x_continuous(label = percent, limits = c(0, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given that you had to take only a single stock and given past performance indicates future performance (more on the assumptions later, but for now: historic performances almost never indicates future performances), you would probably choose Google as your best investment choice.
However, the exclusive option is not realistic as there is something called diversification and you can reduce risk by buying multiple stocks.&lt;/p&gt;
&lt;p&gt;Coming back to the examples from the beginning: If you had the choice between three fictive investments A (expected return of 5%, volatility of 4%), B (exp ret of 5%, volatility of 5%), or C (exp ret 6%, volatility of 5%), you would prefer A over B (we say A is efficient to B) as it has less volatility, and C over B as it has a higher expected return.
However, we cannot compare A and C withouth further knowledge.
In summary, the higher a point on the graph is to the left, the more favorable an asset is (higher expected return and less volatility).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;risk-tradeoffs.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we combine assets into a new portfolio that contains stocks from two companies, our portfolio also has an expected risk-return tradeoff.
The crux is to calculate these values for a given set of (multiple) assets and find an optimum portfolio.
This is what we will do next.
We will start with just two assets to show the main principles, then we will add a third asset to show how to find the so-called efficient frontier, lastly we will look into different options and constraints when calculating the efficient frontier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-the-risk-return-tradeoff-of-a-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating the Risk-Return Tradeoff of a Portfolio&lt;/h2&gt;
&lt;div id=&#34;two-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two Assets&lt;/h3&gt;
&lt;p&gt;Given a portfolio consisting of two assets with a share of stock x of &lt;span class=&#34;math inline&#34;&gt;\(\omega_x\)&lt;/span&gt; and a share of stock y of &lt;span class=&#34;math inline&#34;&gt;\(\omega_y = 1 - \omega_x\)&lt;/span&gt; (as we invest all of the cash, we get the constraint that the weights add up to one, so that &lt;span class=&#34;math inline&#34;&gt;\(0 \leq \omega_x \leq 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \leq \omega_y \leq 1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\omega_x + \omega_y = 1\)&lt;/span&gt;).
Take an equally weighted portfolio as an example, where stock x (say Google) makes 50% of the portfolio (&lt;span class=&#34;math inline&#34;&gt;\(\omega_x = 0.5\)&lt;/span&gt;) and stock y (i.e., IBM) makes up for the other 50% of the value (&lt;span class=&#34;math inline&#34;&gt;\(\omega_y = 1 - \omega_x = 0.5\)&lt;/span&gt;).
What are the expected returns and volatility of the portfolio containing the two assets?
Is it a direct linear combination of the two? Not necessarily.
The expected values are a function of the correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; between the returns of the assets.
If the returns are perfectly correlated (&lt;span class=&#34;math inline&#34;&gt;\(\rho = 1\)&lt;/span&gt;), a portfolio would lie on this line, however, the returns are most likely not perfectly correlated.
The next plot shows the possible values a portfolio will take between two simulated return series given a certain correlation, the maths behind calculating this will be explained in a bit).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;correlation_example.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the lower the correlation between two stocks is, the better it is for diversification purposes (remember usuallyy higher expected return (up) and lower volatility (left) is better).
In the case of a perfect negative correlation (&lt;span class=&#34;math inline&#34;&gt;\(\rho = - 1\)&lt;/span&gt;) we could diversify all risk away (of course, reality has most likely no cases where two assets have a perfect positive or negative correlation).&lt;/p&gt;
&lt;p&gt;To calculate the expected return &lt;span class=&#34;math inline&#34;&gt;\(\hat{r}_p\)&lt;/span&gt; and the expected volatility &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p\)&lt;/span&gt; of a portfolio, we need to use the following formulae, wich include the weights &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; for asset x and y, the expected returns &lt;span class=&#34;math inline&#34;&gt;\(\hat{r}\)&lt;/span&gt;, the expected risk &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (standard deviations), as well as the covariance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{x,y}\)&lt;/span&gt; (which is closely related to the correlation) for the two stocks.&lt;/p&gt;
&lt;p&gt;We get the expected return for the portfolio with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{r}_p = \omega_x \hat{r}_x+ \omega_y \hat{r}_y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the expected volatility with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma_p = \sqrt{\omega_x^2 \sigma_x^2 + \omega_y^2 \sigma_y^2 + 2 \omega_x \omega_y \sigma_{x,y}} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given that the correlation between two series can be expressed as a product of their covariance and their respective standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{x,y} = \rho_{x,y} \sigma_x \sigma_y\)&lt;/span&gt;), we can see why the portfolio is relying on the correlation between the inputs.&lt;/p&gt;
&lt;p&gt;I simulated returns for multiple assets already (the code can be obtained here: &lt;a href=&#34;https://github.com/DavZim/Efficient_Frontier/blob/master/R/create_multiple_asset_dataset.R&#34;&gt;create_multiple_asset_dataset.R&lt;/a&gt;).
Using the maths we can calculate the expected return and volatility for a portfolio for given shares.
As we want to see how good our portfolio is, we calculate not one, but many possible portfolios that have a share of &lt;span class=&#34;math inline&#34;&gt;\(0\leq \omega_x \leq 1\)&lt;/span&gt; (Given that we cannot borrow money, we have the restriction &lt;span class=&#34;math inline&#34;&gt;\(\omega_x + \omega_y = 1\)&lt;/span&gt;, thus we only have &lt;span class=&#34;math inline&#34;&gt;\(\omega_x\)&lt;/span&gt; as a variable and not &lt;span class=&#34;math inline&#34;&gt;\(\omega_y\)&lt;/span&gt;).
First we have to load the returns, then we calculate the necessary values, in the last step we calculate the expected values for the portfolio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the data
link &amp;lt;- &amp;quot;https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/mult_assets.csv&amp;quot;
mult_assets &amp;lt;- fread(link)

# calculate the necessary values:
# I) expected returns for the two assets
er_x &amp;lt;- mean(mult_assets$x)
er_y &amp;lt;- mean(mult_assets$y)

# II) risk (standard deviation) as a risk measure
sd_x &amp;lt;- sd(mult_assets$x)
sd_y &amp;lt;- sd(mult_assets$y)

# III) covariance
cov_xy &amp;lt;- cov(mult_assets$x, mult_assets$y)

# create 1000 portfolio weights (omegas)
x_weights &amp;lt;- seq(from = 0, to = 1, length.out = 1000)

# create a data.table that contains the weights for the two assets
two_assets &amp;lt;- data.table(wx = x_weights,
                         wy = 1 - x_weights)

# calculate the expected returns and standard deviations for the 
# 1000 possible portfolios
two_assets[, &amp;#39;:=&amp;#39; (er_p = wx * er_x + wy * er_y,
                   sd_p = sqrt(wx^2 * sd_x^2 +
                                 wy^2 * sd_y^2 +
                                 2 * wx * (1 - wx) * cov_xy))]
two_assets&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                wx          wy       er_p       sd_p
##    1: 0.000000000 1.000000000 0.03000000 0.01973497
##    2: 0.001001001 0.998998999 0.03004001 0.01971606
##    3: 0.002002002 0.997997998 0.03008001 0.01969728
##    4: 0.003003003 0.996996997 0.03012002 0.01967863
##    5: 0.004004004 0.995995996 0.03016002 0.01966010
##   ---                                              
##  996: 0.995995996 0.004004004 0.06980549 0.04979363
##  997: 0.996996997 0.003003003 0.06984550 0.04984333
##  998: 0.997997998 0.002002002 0.06988550 0.04989305
##  999: 0.998998999 0.001001001 0.06992551 0.04994277
## 1000: 1.000000000 0.000000000 0.06996551 0.04999250&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lastly plot the values
ggplot() +
  geom_point(data = two_assets, aes(x = sd_p, y = er_p, color = wx)) +
  geom_point(data = data.table(sd = c(sd_x, sd_y), mean = c(er_x, er_y)),
             aes(x = sd, y = mean), color = &amp;quot;red&amp;quot;, size = 3, shape = 18) +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Possible Portfolios with Two Risky Assets&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, 
                     limits = c(0, max(two_assets$er_p) * 1.2)) +
  scale_x_continuous(label = percent, 
                     limits = c(0, max(two_assets$sd_p) * 1.2)) +
  scale_color_continuous(name = expression(omega[x]), labels = percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the color coding we can see that the upper point stands for stock x (&lt;span class=&#34;math inline&#34;&gt;\(\omega_x = 100%\)&lt;/span&gt;) and the lower point stands for stock y (&lt;span class=&#34;math inline&#34;&gt;\(\omega_x = 0%\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(\omega_y = 100%\)&lt;/span&gt;).
We also see that all possible portfolios lay on a curve.
That means the line stands for all possible portfolios combining the two assets.
This will change if we add a third asset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-third-asset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a Third Asset&lt;/h3&gt;
&lt;p&gt;Adding a third asset to the portfolio expands the formula.
For the portfolio we get an expected return of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{r}_p = \omega_x \hat{r}_x + \omega_y \hat{r}_y + \omega_z \hat{r}_z
\]&lt;/span&gt;
and an expected standard deviation of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma_p = \sqrt{\omega_x^2 \sigma_x^2 + \omega_y^2 \sigma_y^2 + \omega_z^2 \sigma_z^2 +
2 \omega_x \omega_y \sigma_{x,y} + 2 \omega_x \omega_z \sigma_{x,z} + 2 \omega_y \omega_z \sigma_{y,z}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Doing this in R is fairly easy once you understand the last code-chunk for two assets.
By expanding it we get the following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate the necessary values:
# I) expected returns for the two assets
er_x &amp;lt;- mean(mult_assets$x)
er_y &amp;lt;- mean(mult_assets$y)
er_z &amp;lt;- mean(mult_assets$z)

# II) risk (standard deviation) as a risk measure
sd_x &amp;lt;- sd(mult_assets$x)
sd_y &amp;lt;- sd(mult_assets$y)
sd_z &amp;lt;- sd(mult_assets$z)

# III) covariance
cov_xy &amp;lt;- cov(mult_assets$x, mult_assets$y)
cov_xz &amp;lt;- cov(mult_assets$x, mult_assets$z)
cov_yz &amp;lt;- cov(mult_assets$y, mult_assets$z)

# create portfolio weights (omegas)
x_weights &amp;lt;- seq(from = 0, to = 1, length.out = 1000)

# create a data.table that contains the weights for the three assets
three_assets &amp;lt;- data.table(wx = rep(x_weights, each = length(x_weights)),
                           wy = rep(x_weights, length(x_weights)))

three_assets[, wz := 1 - wx - wy]


# calculate the expected returns and standard deviations for the 1000 possible portfolios
three_assets[, &amp;#39;:=&amp;#39; (er_p = wx * er_x + wy * er_y + wz * er_z,
                     sd_p = sqrt(wx^2 * sd_x^2 +
                                   wy^2 * sd_y^2 +
                                   wz^2 * sd_z^2 +
                                   2 * wx * wy * cov_xy +
                                   2 * wx * wz * cov_xz +
                                   2 * wy * wz * cov_yz))]

# take out cases where we have negative weights (shortselling)
three_assets &amp;lt;- three_assets[wx &amp;gt;= 0 &amp;amp; wy &amp;gt;= 0 &amp;amp; wz &amp;gt;= 0] 
three_assets&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               wx          wy           wz       er_p       sd_p
##      1: 0.000000 0.000000000 1.000000e+00 0.04000000 0.03005254
##      2: 0.000000 0.001001001 9.989990e-01 0.03998999 0.03002259
##      3: 0.000000 0.002002002 9.979980e-01 0.03997998 0.02999265
##      4: 0.000000 0.003003003 9.969970e-01 0.03996997 0.02996272
##      5: 0.000000 0.004004004 9.959960e-01 0.03995996 0.02993281
##     ---                                                        
## 500348: 0.996997 0.003003003 4.336809e-17 0.06984550 0.04984333
## 500349: 0.997998 0.000000000 2.002002e-03 0.06990552 0.04989176
## 500350: 0.997998 0.001001001 1.001001e-03 0.06989551 0.04989239
## 500351: 0.998999 0.000000000 1.001001e-03 0.06993552 0.04994212
## 500352: 1.000000 0.000000000 0.000000e+00 0.06996551 0.04999250&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lastly plot the values
ggplot() +
  geom_point(data = three_assets, aes(x = sd_p, y = er_p, color = wx - wz)) +
  geom_point(data = data.table(sd = c(sd_x, sd_y, sd_z), mean = c(er_x, er_y, er_z)),
             aes(x = sd, y = mean), color = &amp;quot;red&amp;quot;, size = 3, shape = 18) +
  # Miscellaneous Formatting
  theme_bw() + 
  ggtitle(&amp;quot;Possible Portfolios with Three Risky Assets&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, limits = c(0, max(three_assets$er_p) * 1.2)) +
  scale_x_continuous(label = percent, limits = c(0, max(three_assets$sd_p) * 1.2)) +
  scale_color_gradientn(colors = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;yellow&amp;quot;),
                        name = expression(omega[x] - omega[z]), labels = percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the area of possible portfolios has expanded into a third dimension.
The colors try to show the two different weights.
A yellow color indicates a portfolio consisting mainly of the asset x, a blue color indicates asset y, and a red area indicates a portfolio of asset z.
We also see the three single asset-portfolios (the three red dots).
I hope now it also becomes clear what an efficient portfolio is.
To give an example: We have many possible portfolios that have a volatility of 2%, but only one of them is a portfolio we would take (the one with the highest expected return) as this is more efficient than the other possible portfolios with a volatility of 2% but with less expected returns.
Therefore we call the (upper) edge of all possible portfolios the efficient frontier.&lt;/p&gt;
&lt;p&gt;Generally speaking, the expected returns and standard deviations for a portfolio consisting of n-assets are
&lt;span class=&#34;math display&#34;&gt;\[
\hat{r}_p = \sum_{i=1}^{n} \omega_i \hat{r}_i
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\sigma_p = \sum_{i=1}^{n} \omega_i^2 \sigma_i^2 + \sum_{i=1}^{n} \sum_{j=1}^{n} \omega_i \omega_j \sigma_{i, j} \forall i \neq j
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-the-efficient-frontier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating the Efficient Frontier&lt;/h2&gt;
&lt;p&gt;The efficient frontier can be calculated on its own without the need to simulate thousands of portfolios and then finding the efficient ones.
We have to distinguish two cases, one with short-selling (that is negative weights) and one without.
We will first look at the case without restrictions (with short-selling allowed).
So far we have restricted our portfolios to only contain positive weights (by filtering out negative weights).&lt;/p&gt;
&lt;div id=&#34;with-short-selling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;With Short-Selling&lt;/h3&gt;
&lt;p&gt;To calculate the efficient frontier we can use the following closed-form formula that calculates the efficient frontier for a given input of risk (the input-value is thus &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;) and for some parameters (&lt;span class=&#34;math inline&#34;&gt;\(\alpha, \beta, \gamma, \delta\)&lt;/span&gt;) using matrix algebra (note, I put the formula here if someone wants to calculate it by hand; if you are just interested in the R code, you can skip the short mathematics section).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{r}_{ef}(\sigma) = \frac{\beta}{\alpha} + \sqrt{\left(\frac{\beta}{\alpha}\right)^2 - \frac{\gamma - \delta * \sigma^2}{\alpha}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is the solution to a quadratic optimization problem.
The parameters are given by the following matrix algebra&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha = 1^T s^{-1} 1,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is a matrix of 1’s with a length of the numbers of stocks, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is a matrix of the covariances between the assets (with a dimension of n times n).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta = 1^T s^{-1} \overline{ret},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\overline{ret}\)&lt;/span&gt; stands for a vector of average returns for each stock.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\gamma = \overline{ret}^T s^{-1} \overline{ret},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and lastly &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\delta = \alpha \gamma - \beta ^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the only inputs are the returns of stocks, in R we can write a short function (&lt;code&gt;calcEFParams&lt;/code&gt;) that calculates the parameters and returns a list with the parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calcEFParams &amp;lt;- function(rets) {
  retbar &amp;lt;- colMeans(rets, na.rm = T)
  covs &amp;lt;- var(rets, na.rm = T) # calculates the covariance of the returns
  invS &amp;lt;- solve(covs)
  i &amp;lt;- matrix(1, nrow = length(retbar))
  
  alpha &amp;lt;- t(i) %*% invS %*% i
  beta &amp;lt;- t(i) %*% invS %*% retbar
  gamma &amp;lt;- t(retbar) %*% invS %*% retbar
  delta &amp;lt;- alpha * gamma - beta * beta
  
  retlist &amp;lt;- list(alpha = as.numeric(alpha),
                  beta = as.numeric(beta),
                  gamma = as.numeric(gamma),
                  delta = as.numeric(delta))
  
  return(retlist)
}

abcds &amp;lt;- calcEFParams(mult_assets)
abcds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $alpha
## [1] 4037.551
## 
## $beta
## [1] 147.8334
## 
## $gamma
## [1] 5.992395
## 
## $delta
## [1] 2339.881&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calcEFValues &amp;lt;- function(x, abcd, upper = T) {
  alpha &amp;lt;- abcd$alpha
  beta &amp;lt;- abcd$beta
  gamma &amp;lt;- abcd$gamma
  delta &amp;lt;- abcd$delta
  
  if (upper) {
    retval &amp;lt;- beta / alpha + sqrt((beta / alpha) ^ 2 - (gamma - delta * x ^ 2) / (alpha))
  } else {
    retval &amp;lt;- beta / alpha - sqrt((beta / alpha) ^ 2 - (gamma - delta * x ^ 2) / (alpha))
  }
  
  return(retval)
}

# calculate the risk-return tradeoff the two assets (for plotting the points)
df_table &amp;lt;- melt(mult_assets)[, .(er = mean(value),
                                  sd = sd(value)), by = variable]

# plot the values
ggplot(df_table, aes(x = sd, y = er)) +
  # add the stocks
  geom_point(size = 4, color = &amp;quot;red&amp;quot;, shape = 18) +
  # add the upper efficient frontier
  stat_function(fun = calcEFValues, args = list(abcd = abcds, upper = T), n = 10000,
                color = &amp;quot;red&amp;quot;, size = 1) +
  # add the lower &amp;quot;efficient&amp;quot; frontier
  stat_function(fun = calcEFValues, args = list(abcd = abcds, upper = F), n = 10000,
                color = &amp;quot;blue&amp;quot;, size = 1) +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Efficient Frontier with Short-Selling&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +
  scale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given the values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; we receive the function for the frontier as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{r}_{ef}(\sigma) = \frac{147.8}{4037.6} \pm \sqrt{\left(\frac{147.8}{4037.6}\right)^2 - \frac{6.0 - 2339.9 * \sigma^2}{4037.6}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The values for the &lt;code&gt;+&lt;/code&gt;-part of the function is the upper, efficient frontier, whereas the &lt;code&gt;-&lt;/code&gt;-part represents the lower, inefficient frontier.&lt;/p&gt;
&lt;p&gt;The red curve (the upper curve) is the real efficient frontier, whereas the lower (blue) curve stands for an inefficient frontier.
This is due to the fact, that we can create a mixture of the three assets that has the same volatility but a higher expected return.
As we are able to shortsell (borrow money by selling stocks that we don’t own and investing this cash) the efficient frontier doesn’t necessarily touch the three assets, nor does it end at the points but extends outwards.&lt;/p&gt;
&lt;p&gt;Although the maths is not trivial (at least when it comes to understanding), the calculation of the efficient frontier is given by a closed mathematical function and its calculation is straightforward.
This will change in the next chapter, when we add the restrictions that all weights have to be larger than zero (i.e., no short-selling).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;without-short-selling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Without Short-Selling&lt;/h3&gt;
&lt;p&gt;Restricting the portfolio selection by only having positive weights of the assets limits the amounts of possible portfolios and introduces complexity that cannot be handled by closed-form mathematics, thus we need to fall back to mathematical optimization.
There is a wide variety of possible packages that are able to do this, I found the &lt;code&gt;portfolio.optim&lt;/code&gt;-function of the &lt;code&gt;tseries&lt;/code&gt;-package most useful.&lt;/p&gt;
&lt;p&gt;The function takes asset returns, and a desired portfolio return as an input (besides other parameters that we do not use here) and returns information about the portfolio at that desired return.
We can write a short wrapper function that gives us the efficient frontier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tseries)

df_table &amp;lt;- melt(mult_assets)[, .(er = mean(value),
                                  sd = sd(value)), by = variable]

er_vals &amp;lt;- seq(from = min(df_table$er), 
               to = max(df_table$er),
               length.out = 1000)

# find an optimal portfolio for each possible possible expected return 
# (note that the values are explicitly set between the minimum and maximum of
# the expected returns per asset)
sd_vals &amp;lt;- sapply(er_vals, function(er) {
  op &amp;lt;- try(portfolio.optim(as.matrix(mult_assets), er), 
            silent = TRUE)
  if (inherits(op, &amp;quot;try-error&amp;quot;)) return(NA)
  return(op$ps)
})

plot_dt &amp;lt;- data.table(sd = sd_vals, er = er_vals)
plot_dt &amp;lt;- plot_dt[!is.na(sd)]

# find the lower and the upper frontier
minsd &amp;lt;- min(plot_dt$sd)
minsd_er &amp;lt;- plot_dt[sd == minsd, er]
plot_dt[, efficient := er &amp;gt;= minsd_er]
plot_dt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              sd         er efficient
##   1: 0.01965715 0.03004001     FALSE
##   2: 0.01958006 0.03008001     FALSE
##   3: 0.01950373 0.03012002     FALSE
##   4: 0.01942815 0.03016002     FALSE
##   5: 0.01935335 0.03020003     FALSE
##  ---                                
## 994: 0.04965690 0.06976548      TRUE
## 995: 0.04972396 0.06980549      TRUE
## 996: 0.04979104 0.06984550      TRUE
## 997: 0.04985816 0.06988550      TRUE
## 998: 0.04992531 0.06992551      TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_point(data = plot_dt[efficient == F], aes(x = sd, y = er), size = 0.5, color = &amp;quot;blue&amp;quot;) +
  geom_point(data = plot_dt[efficient == T], aes(x = sd, y = er), size = 0.5, color = &amp;quot;red&amp;quot;) +
  geom_point(data = df_table, aes(x = sd, y = er), size = 4, color = &amp;quot;red&amp;quot;, shape = 18) +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Efficient Frontier without Short-Selling&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +
  scale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Comparing the two scenarios we see that restricting the weights (prohibiting short-sells), we also restrict the number of possible portfolios.
In some cases, restricting short-sells yields portfolios with less expected returns or a higher expected volatility.&lt;/p&gt;
&lt;p&gt;To directly compare the two options we can use the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# combine the data into one plotting data.table called &amp;quot;pdat&amp;quot;
# use plot_dt with constraints
pdat1 &amp;lt;- plot_dt[, .(sd, er, type = &amp;quot;wo_short&amp;quot;, efficient)]

# calculate the values without constraints
pdat2lower &amp;lt;- data.table(sd = seq(from = 0, to = max(pdat1$sd) * 1.2, length.out = 1000))
pdat2lower[, &amp;#39;:=&amp;#39; (er = calcEFValues(sd, abcds, F),
                   type = &amp;quot;short&amp;quot;,
                   efficient = F)]

pdat2upper &amp;lt;- data.table(sd = seq(from = 0, to = max(pdat1$sd) * 1.2, length.out = 1000))
pdat2upper[, &amp;#39;:=&amp;#39; (er = calcEFValues(sd, abcds, T),
                   type = &amp;quot;short&amp;quot;,
                   efficient = T)]

pdat &amp;lt;- rbindlist(list(pdat1, pdat2upper, pdat2lower))

# plot the values
ggplot() +
  geom_line(data = pdat, aes(x = sd, y = er, color = type, linetype = efficient), size = 1) +
  geom_point(data = df_table, aes(x = sd, y = er), size = 4, color = &amp;quot;red&amp;quot;, shape = 18) +
  # Miscellaneous Formatting
  theme_bw() + ggtitle(&amp;quot;Efficient Frontiers&amp;quot;) +
  xlab(&amp;quot;Volatility&amp;quot;) + ylab(&amp;quot;Expected Returns&amp;quot;) +
  scale_y_continuous(label = percent, limits = c(0, max(df_table$er) * 1.2)) +
  scale_x_continuous(label = percent, limits = c(0, max(df_table$sd) * 1.2)) +
  scale_color_manual(name = &amp;quot;Short-Sells&amp;quot;, values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), labels = c(&amp;quot;Allowed&amp;quot;, &amp;quot;Prohibited&amp;quot;)) +
  scale_linetype_manual(name = &amp;quot;Efficient&amp;quot;, values = c(2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-05-24-gentle-capm-i_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are many ways to do this and I am certainly not claiming that this is the only option for portfolio selection, but the efficient frontier is a very useful tool to understand one of the basics of finance.
The question of which portfolio of the efficient frontier is considered best within the model-frameworks, has to be left open in this part, but will be adressed using CAPM in part II.&lt;/p&gt;
&lt;p&gt;This closes the first part of the series.
The next section will look more into the CAPM, which is very closely related to the efficient frontier (many would argue that the efficient frontier is part of the CAPM) and its implications, assumptions, and drawbacks.
In the meantime, if you have questions, feedback or issues, please leave a comment or write me an email (you find my email in the about-section).&lt;/p&gt;
&lt;p&gt;Lastly, if you want to use the graphics or code for your own teaching, please contact me and make sure that you give credit and link back to my blog.
Thank you very much.
Also as a disclaimer, I am in no way giving any financial advice in this post.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;One of the many issues I haven’t properly addressed in this entry is the calculation of returns.
There are a couple of ways of how to calculate the returns, most often used are arithmetic and logarithmic returns.&lt;/p&gt;
&lt;p&gt;Arithmetic Returns
&lt;span class=&#34;math display&#34;&gt;\[
r_t = \frac{P_t - P_{t-1}}{P_{t-1}} = \frac{P_t}{P_{t-1}} - 1
\]&lt;/span&gt;
Logarithmic Returns
&lt;span class=&#34;math display&#34;&gt;\[
r_t = log\left(\frac{P_t}{P_{t-1}}\right) = log(P_t) - log(P_{t-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difference between the two is enough to cover another blog post, which is what Pat of PortfolioProbe already did, so if you want to know more about the two, check out his &lt;a href=&#34;http://www.portfolioprobe.com/2010/10/04/a-tale-of-two-returns/&#34;&gt;“Tale of Two Returns”&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating backtests of stock returns using Monte-Carlo and snowfall in parallel</title>
      <link>/blog/2015-09-23-backtests-stock-returns/</link>
      <pubDate>Wed, 23 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/blog/2015-09-23-backtests-stock-returns/</guid>
      <description>


&lt;p&gt;You could say that the following post is an answer/comment/addition to &lt;a href=&#34;http://www.quintuitive.com/2015/09/19/when-is-a-backtest-too-good-to-be-true-part-two/&#34;&gt;Quintuitive&lt;/a&gt;, though I would consider it as a small introduction to parallel computing with &lt;code&gt;snowfall&lt;/code&gt; using the thoughts of Quintuitive as an example.
A quick recap: Say you create a model that is able to forecast 60% of market directions (that is, in 6 out of 10 cases you can predict the direction of the market or an asset for a respective time-period), how well would you do using this strategy?
Hint: if you have a model that is able to predict 60% out-of-sample (think test dataset instead of training dataset) please make sure to share the strategy with me! :)
There are multiple ways to do it, I will show you how to simulate multiple cases using real-life financial data from the German Dax index, Monte-Carlo techniques, and parallel computing using the &lt;code&gt;snowfall&lt;/code&gt;-package of the R language.&lt;/p&gt;
&lt;p&gt;The piece is structured as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load financial data using quantmod&lt;/li&gt;
&lt;li&gt;Show one simulation case with a probability of 51%&lt;/li&gt;
&lt;li&gt;Simulate n cases for one probability and average the result&lt;/li&gt;
&lt;li&gt;Simulate k different cases with different probabilities&lt;/li&gt;
&lt;li&gt;Use the snowfall-package to use parallel techniques&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;load-financial-data-using-quantmod&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Load Financial Data using Quantmod&lt;/h1&gt;
&lt;p&gt;To have some data we look at the German Dax index (&lt;code&gt;^GDAXI&lt;/code&gt;), which consists of the 30 largest companies in Germany. We can load the data and transform it to a dataframe like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(quantmod)
 
# download DAX data from Yahoo
dax &amp;lt;- getSymbols(&amp;quot;^GDAXI&amp;quot;, from = &amp;quot;2000-01-01&amp;quot;, auto.assign = F)
 
# create a data.frame Sdata, that contains the stock data
Sdata &amp;lt;- data.frame(date = index(dax), price = as.numeric(Ad(dax)))
 
# calculate returns
Sdata$rets &amp;lt;- Sdata$price / c(NA, Sdata$price[1:(nrow(Sdata) - 1)]) - 1
 
head(Sdata)
#         date   price          rets
# 1 2010-01-04 6048.30            NA
# 2 2010-01-05 6031.86 -0.0027181096
# 3 2010-01-06 6034.33  0.0004095279
# 4 2010-01-07 6019.36 -0.0024808413
# 5 2010-01-08 6037.61  0.0030318839
# 6 2010-01-11 6040.50  0.0004786889
 
# create a first plot to have a look at the price development
plot(x = Sdata$date, y = Sdata$price, type = &amp;quot;l&amp;quot;, main = &amp;quot;Dax Development&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;dax_dev_init.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-one-simulation-case-with-a-probability-of-51&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Show one simulation case with a probability of 51%&lt;/h1&gt;
&lt;p&gt;Now that we have some data, we create a function &lt;code&gt;get.fprice&lt;/code&gt; that takes in three arguments: the returns of an asset, the percentage of right predictions, and an initial price of the investment (or just the first price of the benchmark).
The function returns a vector of values of the investment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get.fprice &amp;lt;- function(rets, perc.right, init.price){
  # 1. sample the goodness of the returns
  good.forecast &amp;lt;- sample(x = c(T, F),
                          size = length(rets),
                          prob = c(perc.right, 1 - perc.right),
                          replace = T)
 
  # 2. get the forecasted directions, the same as the true rets
  # if good.forecast = T
  dir &amp;lt;- ifelse(rets &amp;gt; 0, 1, -1)
  forecast.dir &amp;lt;- ifelse(good.forecast, dir, -dir)
  # if the percentage sampled should be displayed
  # mean(dir == forecast.dir, na.rm = T) 
   
  # 3. calculate the return of the forecast
  forecast.ret &amp;lt;- forecast.dir * rets
 
  # 4. calculate the prices
  shift.forecast.ret &amp;lt;- forecast.ret[2:length(forecast.ret)]
  forecast.price &amp;lt;- cumprod(1 + shift.forecast.ret) * init.price
  forecast.price &amp;lt;- c(init.price, forecast.price)
  return(forecast.price)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this function we are able to create a single simulation for one probability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set a seed for reproducability
set.seed(42)
 
# simulate one series of prices
Sdata$fprice &amp;lt;- get.fprice(rets = Sdata$rets, perc.right = 0.51,
                           init.price = Sdata$price[1])
 
# plot the two developments
plot(x = Sdata$date, y = Sdata$price, type = &amp;quot;l&amp;quot;, 
     main = &amp;quot;Dax vs. Forecast&amp;quot;, xlab = &amp;quot;Date&amp;quot;, ylab = &amp;quot;Price&amp;quot;,
     ylim = c(0, max(Sdata$price, Sdata$fprice)))
lines(x = Sdata$date, y = Sdata$fprice, col = &amp;quot;red&amp;quot;)
legend(&amp;quot;topleft&amp;quot;, c(&amp;quot;Dax&amp;quot;, paste(perc.right, &amp;quot;forecast&amp;quot;)), 
       col = 1:2, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;51_single_forecast.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you play around with the seed, you will see that not all cases show a similar picture.
This is something inherent to simulations.
To avoid being overly dependent on the seed value for the random number generation, we use the law of large numbers and simulate not one but multiple cases and use the average as a result.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-n-cases-for-one-probability-and-average-the-result&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Simulate n cases for one probability and average the result&lt;/h1&gt;
&lt;p&gt;Monte-Carlo based simulations are multiple simulation of random developments.
With the next code-snippet we can simulate n cases per probability (i.e., we take n simulated paths and average them).
First we create a function &lt;code&gt;get.fprice.n&lt;/code&gt; that works like the &lt;code&gt;get.fprice&lt;/code&gt;-function, but creates not one but &lt;code&gt;n = 10000&lt;/code&gt; cases using the apply-function family.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get.fprice.n &amp;lt;- function(rets, perc.right, init.price, n){
  # create a function that produces the goodness of the forecast
  get.good.forecast &amp;lt;- function(x){
    good.forecast &amp;lt;- sample(x = c(T, F),
                            size = length(rets),
                            prob = c(perc.right, 1 - perc.right),
                            replace = T)
    return(good.forecast)
  }
   
  # 1. sample the goodness of the returns
  good.forecasts &amp;lt;- sapply(1:n, get.good.forecast)
   
  # 2. get the forecasted directions, the same as the true rets 
  # if good.forecast = T
  dir &amp;lt;- ifelse(rets &amp;gt; 0, 1, -1)
  forecast.dirs &amp;lt;- apply(good.forecasts, 2, function(x) {
    ifelse(x, dir, -dir)
  })
    
  # 3. calculate the return of the forecast
  forecast.rets &amp;lt;- forecast.dirs * rets
   
  # 4. calculate the prices
  forecast.prices &amp;lt;- apply(forecast.rets, 2, function(x) {
    cumprod(1 + x[2:length(x)]) * init.price
  })
   
  forecast.prices &amp;lt;- rbind(rep(init.price, ncol(forecast.prices)),
                           forecast.prices)
   
  # collapse the n simulations to just one by taking the average
  forecast.price &amp;lt;- apply(forecast.prices, 1, mean)
  return(forecast.price)
}
 
# simulate 10.000 cases 
# set a seed for reproducability, 
# should not matter due to the Law Of Large Numbers
set.seed(42)
 
# simulate 10.000 series of prices
t &amp;lt;- Sys.time()
Sdata$fprice &amp;lt;- get.fprice.n(rets = Sdata$rets, 
                             perc.right = 0.51,
                             init.price = Sdata$price[1],
                             n = 10000)
Sys.time() - t # takes 5.69257 seconds on my machine
 
# plot the two developments
plot(x = Sdata$date, y = Sdata$price, type = &amp;quot;l&amp;quot;, 
     main = &amp;quot;Dax vs. Forecasts&amp;quot;, xlab = &amp;quot;Date&amp;quot;, ylab = &amp;quot;Price&amp;quot;,
     ylim = c(0, max(Sdata$price, Sdata$fprice)))
lines(x = Sdata$date, y = Sdata$fprice, col = &amp;quot;red&amp;quot;)
legend(&amp;quot;topleft&amp;quot;, c(&amp;quot;Dax&amp;quot;, &amp;quot;aggregated forecasts&amp;quot;), 
       col = 1:2, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;aggregated_forecasts.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-k-different-cases-with-different-probabilities&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Simulate k different cases with different probabilities&lt;/h1&gt;
&lt;p&gt;You may want to compare not just one case with 51% but maybe 2, 5 or 100 cases (k) if the time allows it.
The following section creates &lt;code&gt;k = 4&lt;/code&gt; cases that range from 45% to 55% of directions predicted correctly.
Each case is simulated &lt;code&gt;n = 10000&lt;/code&gt; times and then averaged.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 4
# the percentages that will be used later on
perc.right &amp;lt;- seq(from = 0.45, to = 0.55, length.out = k)
perc.right
#&amp;gt; [1] 0.4500000 0.4833333 0.5166667 0.5500000
 
# simulate k cases n times, equals 40.000 times
t &amp;lt;- Sys.time()
forecasted.prices &amp;lt;- sapply(perc.right, function(x) {
  get.fprice.n(rets = Sdata$rets, 
               perc.right = x,
               init.price = Sdata$price[1],
               n = 10000)
})
Sys.time() - t # takes 21.592 seconds on my machine
 
# plot the results
plot(x = Sdata$date, y = Sdata$price, type = &amp;quot;l&amp;quot;, 
     main = &amp;quot;Dax vs. Forecasts&amp;quot;, xlab = &amp;quot;Date&amp;quot;, ylab = &amp;quot;Price&amp;quot;,
     ylim = c(0, max(forecasted.prices, Sdata$price)))
for (i in 1:k){
  lines(x = Sdata$date, y = forecasted.prices[, i], col = (i + 1))
}
legend(&amp;quot;topleft&amp;quot;, c(&amp;quot;Dax&amp;quot;, 
                    paste0(&amp;quot;P = &amp;quot;, round(perc.right, 2), sep = &amp;quot;)), 
       col = 1:(k + 1), lty = 1, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;4_cases1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-the-snowfall-package-to-use-parallel-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Use the snowfall-package to use parallel techniques&lt;/h1&gt;
&lt;p&gt;Now to the interesting part - using parallel techniques to reduce the computation time (time to beat: ~21 seconds).
There are many packages out there in the “R-osphere”.
I found the &lt;code&gt;snowfall&lt;/code&gt; package most useful and easiest to use.
If you want to get a broader picture of the possibilities you should head over to &lt;a href=&#34;http://de.slideshare.net/bytemining/taking-r-to-the-limit-high-performance-computing-in-r-part-1-parallelization-la-r-users-group-727&#34;&gt;Ryan’s slides&lt;/a&gt;, which give a comprehensive analysis of high-performance computing in R.&lt;/p&gt;
&lt;p&gt;My simplified understanding of parallel computing so far is that there is one master node (think of it as a departement head that coordinates the workers) and multiple slave nodes (the workers).
The master node then distributes tasks to each slave that computes the result and sends it back to the master.&lt;/p&gt;
&lt;p&gt;In snowfall we first have to specify the number of slaves by initiating the clusters with the &lt;code&gt;sfInit&lt;/code&gt; command.
As each slave starts a new environment, we have to export the libraries, functions, and datasets that the slaves will use by calling the &lt;code&gt;sfExport&lt;/code&gt;-function (alternative we can call &lt;code&gt;sfExportAll&lt;/code&gt;, which exports all elements in the current environment).
&lt;code&gt;Snowfall&lt;/code&gt; has it’s own set of functions that work very similar to the apply-family, which we call to hand over the computation to the master node.
To be more precise we call &lt;code&gt;sfClusterApplyLB&lt;/code&gt; (LB stands for load balancing).
As a last step we have to stop the cluster by calling &lt;code&gt;sfStop&lt;/code&gt;.
Easy, right?&lt;/p&gt;
&lt;p&gt;The full snowfall code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the library
library(snowfall)
 
# initiate the data
k &amp;lt;- 4
perc.right &amp;lt;- seq(from = 0.45, to = 0.55, length.out = k)
 
# initiate the clusters, in this case 4 slaves
sfInit(parallel = T, cpus = 4)
# if you are unsure how many cpus you can use, try
# ?parallel::detectCores
# parallel::detectCores(logical = T)
 
# export the necessary data and functions to each cluster
sfExport(&amp;quot;get.fprice.n&amp;quot;, &amp;quot;Sdata&amp;quot;)
 
t &amp;lt;- Sys.time()
# use the snowfall cluster-apply, which works similar to sapply 
# note that I used sfClusterApplyLB and not sfClusterApply 
# the LB stands for Load-Balancing
result &amp;lt;- sfClusterApplyLB(perc.right,  function(x){
  get.fprice.n(rets = Sdata$rets, 
               perc.right = x,
               init.price = Sdata$price[1],
               n = 10000)
})
Sys.time() - t 
#&amp;gt; 9.861 seconds
# IMPORTANT: Stop the cluster so it doesn&amp;#39;t messes around in the background
sfStop()
 
# have a look at the data
str(result)
#&amp;gt; List of 4
#&amp;gt; $ : num [1:1463] 6048 6047 6046 6045 6043 ...
#&amp;gt; $ : num [1:1463] 6048 6048 6048 6047 6047 ...
#&amp;gt; $ : num [1:1463] 6048 6049 6049 6049 6050 ...
#&amp;gt; $ : num [1:1463] 6048 6050 6050 6052 6053 ...
 
# convert to a data.frame
forecasted.prices &amp;lt;- data.frame(matrix(unlist(result), ncol = length(result)))
head(forecasted.prices)
#&amp;gt; X1       X2       X3       X4
#&amp;gt; 1 6048.300 6048.300 6048.300 6048.300
#&amp;gt; 2 6046.659 6047.672 6048.888 6050.085
#&amp;gt; 3 6046.424 6047.589 6048.989 6050.323
#&amp;gt; 4 6045.218 6047.276 6049.403 6051.897
#&amp;gt; 5 6043.136 6046.546 6049.886 6053.465
#&amp;gt; 6 6042.890 6046.399 6049.992 6053.763
 
# and finally the last plot
plot(x = Sdata$date, y = Sdata$price, type = &amp;quot;l&amp;quot;, 
     main = &amp;quot;Snowfall Forecasts&amp;quot;, xlab = &amp;quot;Date&amp;quot;, ylab = &amp;quot;Price&amp;quot;,
     ylim = c(0, max(forecasted.prices, Sdata$price)))
for (i in 1:k){
  lines(x = Sdata$date, y = forecasted.prices[, i], col = (i + 1))
}
legend(&amp;quot;topleft&amp;quot;, c(&amp;quot;Dax&amp;quot;, 
                    paste0(&amp;quot;P = &amp;quot;, round(perc.right, 2), sep = &amp;quot;)), 
       col = 1:(k + 1), lty = 1, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;snowfall_sim_forecasts.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot shows basically the same picture compared to the last picture.
Everything else would be very strange.&lt;/p&gt;
&lt;p&gt;Using parallel techniques, we can reduce the time for the computations from 21 seconds to 9 seconds.
If we increase the number of cases (n = 100,000 instead of 10,000) then we should see that the parallel computation takes only a quarter of the time with 4 clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;A few lines of code shows us that even a small advantage (being able to forecast 5%p more correct Dax movements than a coin) can lead to monstrous returns of roughly 30% p.a. (CAGR of an initial investment of 6,050 and a value of 23,480 after 5.5 years).
However, the take-away should be that if you find such a model, make sure that you backtest the hell out of it before telling anyone about it.
Ex post, everyone is a billionaire.
Another take-away and the purpose of this post is how to use Monte-Carlo and parallel computing techniques to create a simple and fast simulation to check something.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outro&lt;/h1&gt;
&lt;p&gt;I am not an expert in parallel computing, so should you find an error in the explanation or in the code, please leave a comment and I will happily correct the error.
Furthermore, if you have any questions, comments or ideas, please leave a comment or send me an email.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
